---
title: "DSCI353-353m-453: Class 05a-p Bootstrap for Mixed Effects Models"
subtitle: "Profs: R. H. French, L. S. Bruckman, P. Leu, K. Davis, S. Cirlos"
author: "TAs: W. Oltjen, K. Hernandez, M. Li, M. Li, D. Colvin" 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: TRUE
    number_sections: TRUE
    toc_depth: 6
    highlight: tango
  html_notebook:
  html_document:
    css: ../lab.css
    highlight: pygments
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: paged
urlcolor: blue
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 5, # the width for plots created by code chunk
  fig.height = 3.5, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = FALSE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
  # options(tinytex.verbose = TRUE)
```

 \setcounter{section}{5}
 \setcounter{subsection}{1}
 \setcounter{subsubsection}{2}

#### Class Readings, Assignments, Syllabus Topics

##### Reading, Lab Exercises, SemProjects

  - Readings: 
    - For today: DLwR1, DL06,07
    - For next class: DLwR2
  - Laboratory Exercises: 
    - LE2 is due next Tuesday
    - LE3 is given out next Tuesday
  - Office Hours: (Class Canvas Calendar for Zoom Link)
    - Wednesdays @ 4:00 PM to 5:00 PM  
    - Saturdays @ 3:00 PM to 4:00 PM
    - **Office Hours are on Zoom, and recorded**
  - Semester Projects
    - Office Hours for SemProjs: Mondays at 4pm on Zoom
    - DSCI 453 Students Biweekly Updates Due 
      - Update # is Due **  **
    - DSCI 453 Students 
      - Next Report Out # is Due ** **
    - All DSCI 353/353M/453, E1453/2453 Students: 
      - Peer Grading of Report Out #1 is Due **  **
    - Exams
      - MidTerm: **Thursday March 9th**, in class or remote, 11:30 - 12:45 PM
      - Final: **Thursday May 4th**, 2023, 12:00PM - 3:00PM, Nord 356 or remote
    
##### Textbooks

- Text Books for DSCI353/353M/453

  - [R4DS: Wickham: R for Data Science](https://r4ds.had.co.nz/)
  - [ISLR: Intro to Statistical Learning with R, 2nd Ed.](https://www.statlearning.com/)
  - DLwR: Deep Learning with R, Chollet, Allaire, 
  - [DLGB: Deep Learning, Goodfellow, Bengio,  Courville](https://www.deeplearningbook.org/)

- Magazine Articles about Deep Learning

  - DL1 to DL12 are "Deep Learning" articles in 3-readings/2-articles/


- Books from DSCI351/351M/451

  - [Peng: R Programming for Data Science](https://leanpub.com/rprogramming)
  - [Peng: Exploratory Data Analysis with R](https://leanpub.com/exdata)
  - [Open Intro Stats, v4](https://openintro.org/os)
  - [R4DS: Wickham: R for Data Science](https://r4ds.had.co.nz/)


##### Tidyverse Cheatsheets, Functions and Reading Your Code

- Look at the Tidyverse Cheatsheet 

  - **Tidyverse For Beginners Cheatsheet**
    - In the Git/20s-dsci353-353m-453-prof/3-readings/3-CheatSheets/ folder
  - **Data Wrangling with dplyr and tidyr Cheatsheet**

  
  Tidyverse Functions & Conventions
  
    - The pipe operator `%>%`
    - Use `dplyr::filter()` to subset data row-wise.
    - Use `dplyr::arrange()`  to sort the observations in a data frame
    - Use `dplyr::mutate()` to update or create new columns of a data frame
    - Use `dplyr::summarize()` to turn many observations into a single data point
    - Use `dplyr::arrange()` to change the ordering of the rows of a data frame 
    - Use `dplyr::select()` to choose variables from a tibble, 
      - keeps only variables you mention
    - Use `dplyr::rename()` keeps all the variables and renames variables
      - rename(iris, petal_length = Petal.Length)
    - These can be combined using `dplyr::group_by()` 
      - which lets you perform operations “by group”. 
    - The `%in%` matches conditions provided by a vector using the c() function
    - The **forcats** package has tidyverse functions 
      - for factors (categorical variables)
    - The **readr** package has tidyverse functions 
      - to read_..., melt_... col_..., parse_... data and objects

Reading Your Code: Whenever you see

  - The assignment operator `<-`, think **"gets"**
  - The pipe operator, `%>%`, think **"then"**

##### Syllabus

![DSCI351-351M-451 Syllabus](./figs/syllabus.png)


#### Bootstrap applied to Mixed Effects Models

  - Bootstrap is one of the most famous resampling techniques 
    - and is very useful way to get confidence intervals 
    - in situations where classical approach (t- or z- tests) would fail.

##### What is Bootstrap?

  - Lets show a simple bootstrap example 
      - using the height of 100 person in the population.

```{r}
set.seed(20151101)
height <- rnorm(100, 175, 6)
```

Now we will resample 

  - with replacement 1000 times 
  - and compute the median:

```{r}
t0 <- median(height)
t <-
  sapply(1:1000, function(x)
    median(sample(
      x = height, size = 100,
      replace = TRUE
    )))
hist(t)
abline(v = t0, col = "orange", lwd = 3)
```

And this is the histogram that we get.

And that’s it, this is the essence of bootstrap: 

  - resampling the observed data with replacement 
    - and computing the statistic of interest 
    - (here the median) 
    - many times on the resampled data 
  - to get a distribution of the statistic of interest. 
  - This distribution of the statistic of interest can then be used to compute, 
    - for example, confidence intervals.

##### When to use Bootstrap?

  - Bootstrap is used to enable inference on the statistic of interest 
    - when the true distribution of this statistic is unknown. 
  - For example in a linear model the parameters of interest 
    - have a known distribution 
    - from which standard errors and formal tests can be performed. 
  - On the other hand for some statistics 
    - (median, differences between two models …), 
    - if the analyst do not want to spend time writing down equations,
  - [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29) might be a great approach 
    - to get standard errors and confidence intervals 
    - from the bootstrapped distribution.

##### When will Bootstrap Fail?

  - There are some situations where bootstrapped will fail: 
    - (i) the statistic of interest is at the edge of the parameter space 
      - (like minimum or maximum), 
      - the bootstrapped distribution does not converge 
        - (as the number of bootstrap sample increase to infinity) 
        - to the true distribution of the statistic. 
    (ii) sample size is small, 
      - bootstrapping will not increase the power of statistical tests. 
      - If you sample too few data to detect an effect of interest, 
      - using bootstrap will not magically solve your problem 
      - even worse the bootstrap approach will perform less well than others.

##### How many bootstrap samples

  - [As much as possible](https://stats.stackexchange.com/questions/86040/rule-of-thumb-for-number-of-bootstrap-samples) will be the answer. 
  - Note that in here we used low numbers to speed up the computations for class

#### Non-parametric and parametric bootstrap using the `boot` package

  - The boot library in R is very convenient 
    - to easily compute confidence intervals from bootstrap samples. 

**Non-parametric bootstrap with boot:**

```{r}
library(boot)
b1 <- boot(
  data = height,
  statistic = function(x, i)
    median(x[i]),
  R = 1000
)
boot.ci(b1)
```

**Parametric bootstrap with boot:**

```{r}
x <- runif(100, -2, 2)
y <- rnorm(100, 1 + 2 * x, 1)
dat <- data.frame(x = x, y = y)
```

##### Lets do a simple example with a linear model

```{r}
m <- lm(y ~ x)
```

We are interested in getting 

  - the confidence intervals 
    - for the coefficient of the model:

```{r}
foo <- function(out) {
  m <- lm(y ~ x, out)
  coef(m)
}
```

The function rgen 

  - generate new response vector from the model:

```{r}
rgen <- function(dat, mle) {
  out <- dat
  out$y <- unlist(simulate(mle))
  return(out)
}
```

Now generate a 1000 bootstrap sample

```{r}
b2 <- boot(
  dat,
  foo,
  R = 1000,
  sim = "parametric",
  ran.gen = rgen,
  mle = m
)
```

Compute the confidence intervals 

  - for the two coefficients:
    - index = 1 and index = 2

```{r}
boot.ci(b2, type = "perc", index = 1)
```

```{r}
boot.ci(b2, type = "perc", index = 2)
```

In the non-parametric case 

  - `boot` expects two arguments in the function returning the statistic of interest: 
    - the first one is the object from which to compute the statistic 
    - and the second is a vector of 
      - index (`i`), 
      - frequencies (`f`) 
      - or weight (`w`) 
      - defining the bootstrap sample. 
  - In our example, `boot` will generate a series of indices (named `i`) 
    - with replacement 
    - and these will be used to subset the original height vector. 
    
In the parametric case the function returning the statistic(s) of interest 

  - only needs one argument: the original dataset. 
  - We then need to supply another function (`ran.gen`) 
    - describing how to generate the new data, 
    - it needs to return an object of the same form as the original dataset. 
  - This random data generating function need two arguments: 
    - the first one is the original dataset 
    - and the second one contain maximum likelihood estimate 
      - for the parameter of interest, 
      - basically a model object. 
    - The new dataset generated by the ran.gen function 
      - will then be passed to the statistics function 
      - to compute the bootstrapped value for the statistic of interest. 
    - It is then straightforward to get 
      - the confidence intervals for the statistic 
      - using `boot.ci`.

#### Bootstrap applied to mixed-effect models

  - Mixed-effect models are rather complex 
    - and the distributions or numbers of degrees of freedom 
    - of various outputs from them (like parameters …) 
    - is not known analytically. 
  - Which is why the author of the `lme4` package recommends 
    - the use of bootstrap to get 
      - confidence intervals around the model parameters, 
        - the predicted values 
      - but also to get p-values from likelihood ratio tests.

In 3-readings/4-MatSci-And-SemProjReadings

  - There is a good example of fixed- and mixed-effects modeling
    - In [Gok et al. - 2017 - Predictive models of poly(ethylene-terephthalate) .pdf]("../3-readings/4-MatSci-And-SemProjReadings/Gok et al. - 2017 - Predictive models of poly(ethylene-terephthalate) .pdf")
  
##### A simple random intercept model:

```{r}
library(lme4)
dat <- data.frame(x = runif(100, -2, 2), ind = gl(n = 10, k = 10))
dat$y <-
  1 + 2 * dat$x + rnorm(10, 0, 1.2)[dat$ind] + rnorm(100, 0, 0.5)
m <- lme4::lmer(y ~ x + (1 | ind), dat)
```

Get the bootstrapped confidence intervals 

  - for the model parameters:

```{r}
b_par <- bootMer(x = m, FUN = fixef, nsim = 200)
boot.ci(b_par, type = "perc", index = 1)
```

```{r}
boot.ci(b_par, type = "perc", index = 2)
```

Or alternatively:

```{r}
confint(
  m,
  parm = c(3, 4),
  method = "boot",
  nsim = 200,
  boot.type = "perc"
)
```

Get the bootstrapped confidence intervals 

  - around the regression curves:

```{r}
new_dat <- data.frame(x = seq(-2, 2, length = 20))
mm <- model.matrix(~ x, data = new_dat)
predFun <- function(.)
  mm %*% fixef(.)
bb <- bootMer(m, FUN = predFun, nsim = 200) #do this 200 times
```

As we did this 200 times 

  - the 95% CI will be bordered 
  - by the 5th and 195th value:

```{r}
bb_se <- apply(bb$t, 2, function(x)
  x[order(x)][c(5, 195)])
new_dat$LC <- bb_se[1,]
new_dat$UC <- bb_se[2,]
new_dat$pred <- predict(m, newdata = new_dat, re.form =  ~ 0)
```

Plot the results

```{r}
plot(y ~ x, dat, pch = 16)
lines(pred ~ x, new_dat, lwd = 2, col = "orange")
lines(LC ~ x, new_dat, lty = 2, col = "orange")
lines(UC ~ x, new_dat, lty = 2, col = "orange")
```
Finally get bootstrapped p-values 

  - from the likelihood ratio test between two models.

Drawing confidence intervals around the regression curves 

  - is tricky due to the random effect estimated values 
    - which comes with there own uncertainty 
    - (have a look at `dotplot(ranef(m,condVar=TRUE))` to see it). 
    
Bootstrapping is an efficient way to take these uncertainties into account 

  - since the random deviates are re-computed for each draw. 
  
Finally getting p-values for the effect of a fixed-effect term 

  - can be done using a parametric bootstrap approach as described here 
  - and implemented in the function `PBmodcomp` 
    - from the `pbkrtest` package. 
  - In the output of `PBmodcomp` 
    - the bootstrapped p-values is in the PBtest line, 
    - the LRT line reports the standard p-value 
      - assuming a chi-square distribution 
      - for the LRT value.

```{r}
library(pbkrtest)
?PBmodcomp
```

#### Links

  - Lionel Hertzog, "Introduction to bootstrap with applications to mixed-effect models", Nov. 25, 2015. 
  - T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition, 2nd ed. New York: Springer-Verlag, 2009 [Online]. Available: https://web.stanford.edu/~hastie/ElemStatLearn/. 
  - Bradley Efron and Robert J Tibshirani, An introduction to the bootstrap, vol. 57. Chapman & Hall/CRC Monographs on Statistics and Applied Probability, 1993 [Online]. Available: https://www.routledge.com/An-Introduction-to-the-Bootstrap/Efron-Tibshirani/p/book/9780412042317
  - D. Bates, M. Mächler, B. Bolker, and S. Walker, “Fitting Linear Mixed-Effects Models using lme4,” Journal of Statistical Software, vol. 67, no. 1, pp. 1–48, 2015, doi: 10.18637/jss.v067.i01. 



 
