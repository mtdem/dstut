---
title: "CWRU DSCI353-353M-453: Class 03a Predictive Analytics  "
subtitle: "Profs: R. H. French, L. S. Bruckman, P. Leu, K. Davis, S. Cirlos"
author: "TAs: W. Oltjen, K. Hernandez, M. Li, M. Li, D. Colvin" 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: TRUE
    number_sections: TRUE
    toc_depth: 6
    highlight: tango
  html_notebook:
  html_document:
    css: ../lab.css
    highlight: pygments
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: paged
urlcolor: blue
always_allow_html: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 5, # the width for plots created by code chunk
  fig.height = 3.5, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = FALSE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
  # options(tinytex.verbose = TRUE)
```

 \setcounter{section}{3}
 \setcounter{subsection}{1}
 \setcounter{subsubsection}{2}
 
#### Class Readings, Assignments, Syllabus Topics

##### Reading, Lab Exercises, SemProjects

  - Readings: 
    - For today: DL03, ISLR3 
    - For next class: DL04, DL05
  - Laboratory Exercises: 
    - LE1 is **Due Saturday at Midnight**
    - LE2 will be given out Thursday Feb. 2nd
      - **LE2 is due Tuesday Feb. 14th**
  - Office Hours: (Class Canvas Calendar for Zoom Link)
    - Wednesdays @ 4:00 PM to 5:00 PM  
    - Saturdays @ 3:00 PM to 4:00 PM
    - **Office Hours are on Zoom, and recorded**
  - Semester Projects
    - **Office Hours for SemProjs: Mondays at 4pm on Zoom**
    - DSCI 453 Students Biweekly Updates Due 
      - Update #1 is Due ** This Friday **
    - DSCI 453 Students 
      - Next Report Out #1 is Due ** Feb. `17th **
    - All DSCI 353/353M/453, E1453/2453 Students: 
      - Peer Grading of Report Out #1 is Due **  **
    - Exams
      - MidTerm: **Thursday March 9th**, in class or remote, 11:30 - 12:45 PM
      - Final: **Thursday May 4th**, 2023, 12:00PM - 3:00PM, Nord 356 or remote
    

##### Textbooks

- Introduction to R and Data Science

  - For R, Coding, Inferential Statistics
    - Peng: R Programming for Data Science
    - Peng: Exploratory Data Analysis with R
  
Textbooks for this class

  - OIS = Diez, Barr, Çetinkaya-Runde: Open Intro Stat v4
  - R4DS = Wickham, Grolemund: R for Data Science
  
Textbooks for DSCI353/353M/453, And in your Repo now

  - ISLR2 = James, Witten, Hastie, Tibshirani: Intro to Statistical Learning with R, 2nd Ed.
  - ESL = Trevor Hastie, Tibshirani, Friedman: Elements of Statistical Learning
  - DLwR = Chollet, Allaire: Deep Learning with R, 2nd Ed.

Magazine Articles about Deep Learning

  - DL1 to DL6 are "Deep Learning" articles in 3-readings/2-articles/

#### Syllabus

![Modeling, Prediction and Machine Learning Syllabus](./figs/syllabus.png) 

##### Tidyverse Cheatsheets, Functions and Reading Your Code

- Look at the Tidyverse Cheatsheet 

  - **Tidyverse For Beginners Cheatsheet**
    - In the Git/20s-dsci353-353m-453-prof/3-readings/3-CheatSheets/ folder
  - **Data Wrangling with dplyr and tidyr Cheatsheet**

  Tidyverse Functions & Conventions
  
    - The pipe operator `%>%`
    - Use `dplyr::filter()` to subset data row-wise.
    - Use `dplyr::arrange()`  to sort the observations in a data frame
    - Use `dplyr::mutate()` to update or create new columns of a data frame
    - Use `dplyr::summarize()` to turn many observations into a single data point
    - Use `dplyr::arrange()` to change the ordering of the rows of a data frame 
    - Use `dplyr::select()` to choose variables from a tibble, 
      - keeps only variables you mention
    - Use `dplyr::rename()` keeps all the variables and renames variables
      - rename(iris, petal_length = Petal.Length)
    - These can be combined using `dplyr::group_by()` 
      - which lets you perform operations “by group”. 
    - The `%in%` matches conditions provided by a vector using the c() function
    - The **forcats** package has tidyverse functions 
      - for factors (categorical variables)
    - The **readr** package has tidyverse functions 
      - to read_..., melt_... col_..., parse_... data and objects

Reading Your Code: Whenever you see

  - The assignment operator `<-`, think **"gets"**
  - The pipe operator, `%>%`, think **"then"**
  

#### What is Statistical (and Machine) Learning 

- We will go far beyond classical inferential statistical methods, 

  - such as linear regression. 
  
As computing power has increased over the last 20 years 

  - many new, highly computational, regression, or “Statistical Learning”, 
  - methods have been developed. 

In particular the last decade has seen a significant expansion 

  - of the number of possible approaches. 

Here we will provide a very applied overview to such modern non-linear methods as 

  - Generalized Additive Models, 
  - Decision (or Regression) Trees, 
  - Boosting, 
  - Bagging and 
  - Support Vector Machines 

As well as more classical linear approaches such as 

  - Logistic Regression, 
  - Linear Discriminant Analysis, 
  - K-Means Clustering and Nearest Neighbors.

At the end of this course you should have 

  - a basic understanding of how all of these methods work 
  - and be able to apply them in real data analyses. 

With the explosion of “Big Data” problems, 

  - statistical learning has become a very hot field in many areas. 

People with statistical learning skills are in high demand!

To this end, approximately one third of the class time 

  - is dedicated to in lab exercises 
  - where the students will work through 
  - the latest methods we have covered, 
  - on their Open Data Science VDI. 

These labs will ensure that every student 

  - has a full understanding of the 
  -  practical and theoretical, aspects of each method.

#### Supervised and Unsupervised Learning

- Two broad families of algorithms will be covered: 

  * Unsupervised learning algorithms
  * Supervised learning algorithms

##### Unsupervised learning 

- In unsupervised learning, 

  - the algorithm will seek to find the structure that organizes unlabeled data. 

##### Supervised learning 

- In supervised learning, 

  - we know the class or the level of some observations of a given target attribute. 

#### Classification and Regresssion Problems

- There are basically two types of problems that predictive modeling deals with: 

  * Classification problems 
  * Regression problems

##### Classification

- In some cases, 

  - we want to predict which group an observation is part of. 

Here, we are dealing with a quality of the observation. 

##### Regression

- In other cases, 

  - we want to predict an observation's level on an attribute. 

Here, we are dealing with a quantity, and this is a regression problem. 

#### The critical role of domain knowledge 

  * in modeling and prediction

Domain knowledge informs and is informed by data understanding. 

  * The understanding of the data 
    - then informs how the data has to be prepared. 

The next step is data modeling, 

  * which can also lead to further data preparation. 

Data models have to be evaluated, 

  * and this evaluation can be informed by field knowledge, 
    - which is also updated through the data mining process. 

Finally, 

  * if the evaluation is satisfactory, 
    - the models are deployed for prediction. 


#### Caveat: For Predictive Analytics

- Of course, predictions are not always accurate, 

  * and some have written about the caveats of data science. 

What do you think about the relationship between 

  * the attributes titled Predictor and Outcome on the following plot? 

![Relationship between Predictor & Outcome](./figs/lpar-caveat.png)

It seems like there is a relationship between the two. 

  * For the statistically inclined, 
    - I tested its significance:
      - r = 0.4195, p = .0024. 
  * The value p is the probability of obtaining a relationship of this strength or stronger 
    - if there is actually no relationship between the attributes. 
  * ( This is the p-value of hypothesis testing, if p<0.05 
    - typically we assert we can reject the null hypothesis)
  * We could conclude that the relationship between these variables 
    - in the population they come from is quite reliable, 
    - **right?**
    
##### No: Lets think about this

- Believe it or not, 

  * the population these observations come from 
    - is that of randomly generated numbers. 
  * We generated a data frame of 50 columns 
    - of 50 randomly generated numbers. 
  * We then examined all the correlations (manually) 
    - and generated a scatterplot of the two attributes 
    - with the largest correlation we found. 
    
##### The code is provided here, 

- We'll use runif()

  * help(runif)
    - The Uniform Distribution
    - Description

These functions provide information about the uniform distribution 

  - on the interval from min to max. 
    - dunif gives the density, 
    - punif gives the distribution function 
    - qunif gives the quantile function and 
    - runif generates random deviates.

```{r,echo=TRUE}
set.seed(1)
DF <- data.frame(matrix(nrow = 50, ncol = 50))
for (i in 1:50)
  DF[, i] <- runif(50)
plot(DF[[2]], DF[[16]], xlab = "Predictor", ylab = "Outcome")
abline(lm(DF[[2]] ~ DF[[16]]))
cor.test(DF[[2]], DF[[16]])
```

In case you want to check it yourself

  - line 1 sets the seed so that you find the same results as we did, 
  - line 2 generates to the data frame, 
  - line 3 fills it with random numbers, column by column, 
  - line 4 generates the scatterplot, 
  - line 5 fits the regression line, and 
  - line 6 tests the significance of the correlation:

Normally we reject the null with a p-value of <0.05

  * i.e. we'll be wrong 5% of the time
    - in a set of 20 trials
  
Here we did 50 trials

  * And cherry picked the best correlation
    - But its all randomly generated numbers
    - There is no predictive or causal relationship
  * And we'd only recognize this if we consider
    - That our p-value 

##### [Bonferroni Correction](https://en.wikipedia.org/wiki/Bonferroni_correction) for multiple comparisons

- How could this relationship happen given that the odds were 2.4 in 1000 ? 

  * Well, think of it; 
    - we correlated all 50 attributes 2 x 2, 
    - which resulted in 2,450 tests 
    - (not considering the correlation of each attribute with itself). 
  * Such spurious correlation was quite expectable. 

The usual p-value threshold below which 

  - we consider a relationship significant is p = 0.05. 

This means that we expect to be wrong once in 20 times.

  * You would be right to suspect that there are other significant correlations 
    - in the generated data frame (there should be approximately 125 of them in total). 
  * This is the reason why we should always correct the number of tests. 
  * In our example, 
    - as we performed 2,450 tests, 
    - our threshold for significance 
    - should be 0.0000204 (0.05 / 2450).
  * This is called the Bonferroni correction.

#### Overfitting: The need for Trainging and Testing Datasets

- Spurious correlations are always a possibility in data analysis 

  * and this should be kept in mind at all times. 

A related concept is that of overfitting. 

  * Overfitting happens, for instance, 
    - when a weak classifier bases its prediction on the noise in data. 
  * We will discuss overfitting when discussing
    - Training datasets for fit a model to
    - Testing datasets for evaluating the goodness of fit
      - when using various types of cross-validation
    - And when evaluating $Predictive, Adjusted, R^2$
    

#### Citations

1. R Core Team. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing, 2014. http://www.R-project.org/.
2. G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning: 2nd Ed., with Applications in R, 2nd ed. 2021 edition. New York: Springer, 2021. 
3. Diez, David M., Christopher D. Barr, and Mine Çetinkaya-Rundel. OpenIntro Statistics: Third Edition. 3 edition. S.l.: OpenIntro, Inc., 2015.
4. Al Sharif, IOM 530 – Applied Modern Statistical Learning Methods, USC 
5. Mayor, Eric. Learning Predictive Analytics with R. Packt Publishing - ebooks Account, 2015.




