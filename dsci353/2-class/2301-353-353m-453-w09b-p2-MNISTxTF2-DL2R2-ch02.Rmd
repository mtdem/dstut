---
title: "DSCI351-351m-451: Class 01a, (CWRU, Pitt, UCF, UTRGV)"
subtitle: "Profs: R. H. French, L. S. Bruckman, P. Leu, K. Davis, S. Cirlos"
author: "TAs: W. Oltjen, K. Hernandez, M. Li, M. Li, D. Colvin" 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: TRUE
    number_sections: TRUE
    toc_depth: 6
    highlight: tango
  html_notebook:
  html_document:
    css: ../lab.css
    highlight: pygments
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: paged
urlcolor: blue
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 6, # the width for plots created by code chunk
  fig.height = 4, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # options(tinytex.verbose = TRUE)
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = TRUE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
```

 \setcounter{section}{8}
 \setcounter{subsection}{1}
 \setcounter{subsubsection}{2}


#### Now lets do the MNIST problem using TensorFlow 2 

Lets check that we have a GPU and TF2 setup

```{r ----setup, include=FALSE} 
tensorflow::as_tensor(1)

```

Load MNIST dataset from Tensorflow

```{r}

library(tensorflow)
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y

```

Lets see the structure of our

  - training images
  - training labels


```{r}
str(train_images)
str(train_labels)

```

Lets see the structure of our

  - test images
  - test labels

```{r}
str(test_images)
str(test_labels)


```

Now we'll build a TF2 model

  - Using the Kearas Layer API
  - To define the Neural Network's Architecture or Structure

```{r}
model <- keras_model_sequential(list(
  layer_dense(units = 512, activation = "relu"),
  layer_dense(units = 10, activation = "softmax")
))

```

The next step is to define the following

  - Which Optimizer
  - Which Loss Function
  - Which Metrics


```{r}
compile(model,
        optimizer = "rmsprop",
        loss = "sparse_categorical_crossentropy",
        metrics = "accuracy")


```

Now we have to prepare our training and test data

  - By Reshaping them appropriately
  - And Rescaling them to be between 0 to 1
  
So we refer to this as **Reshape/Rescale** step

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255


```

Now lets train our NN model

  - Using Stochastic Gradient Descent
  - For Five Epochs
  - Using a Batch Size of 128 images


```{r}
fit(model, train_images, train_labels, epochs = 5, batch_size = 128)

```



```{r}
test_digits <- test_images[1:10, ]
predictions <- predict(model, test_digits)
str(predictions)
predictions[1, ]


```



```{r}
which.max(predictions[1, ])
predictions[1, 8]


```



```{r}
test_labels[1]


```



```{r}
metrics <- evaluate(model, test_images, test_labels)
metrics["accuracy"]


```



```{r}
x <- as.array(c(12, 3, 6, 14, 7))
str(x)
length(dim(x))


```



```{r}
x <- array(seq(3 * 5), dim = c(3, 5))
x
dim(x)


```



```{r}
x <- array(seq(2 * 3 * 4), dim = c(2, 3, 4))
str(x)
length(dim(x))


```



```{r}
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y


```



```{r}
length(dim(train_images))


```



```{r}
dim(train_images)


```



```{r}
typeof(train_images)


```



```{r}
digit <- train_images[5, , ]
par(mar = c(0, 0, 0, 0))
plot(as.raster(abs(255 - digit), max = 255))


```



```{r}
train_labels[5]


```



```{r}
my_slice <- train_images[10:99, , ]
dim(my_slice)


```



```{r}
my_slice <- train_images[, 15:28, 15:28]
dim(my_slice)


```



```{r}
batch <- train_images[1:128, , ]


```



```{r}
batch <- train_images[129:256, , ]


```



```{r}
n <- 3
batch <- train_images[seq(to = 128 * n, length.out = 128), , ]


```



```{r}
layer_dense(units = 512, activation = "relu")


## ---- eval = FALSE--------------------------------------------------------
## output <- relu(dot(W, input) + b)


```



```{r}
naive_relu <- function(x) {
  stopifnot(length(dim(x)) == 2)
  for (i in 1:nrow(x))
    for (j in 1:ncol(x))
      x[i, j] <- max(x[i, j], 0)
  x
}


```



```{r}
naive_add <- function(x, y) {
  stopifnot(length(dim(x)) == 2, dim(x) == dim(y))
  for (i in 1:nrow(x))
    for (j in 1:ncol(x))
      x[i, j]  <- x[i, j] + y[i, j]
  x
}


## ---- eval = FALSE--------------------------------------------------------
## z <- x + y
## z[z < 0] <- 0


```



```{r}
random_array <- function(dim, min = 0, max = 1)
  array(runif(prod(dim), min, max),
        dim)

x <- random_array(c(20, 100))
y <- random_array(c(20, 100))

system.time({
  for (i in seq_len(1000)) {
    z <- x + y
    z[z < 0] <- 0
  }
})[["elapsed"]]


```



```{r}
naive_add <- function(x, y) {
  stopifnot(length(dim(x)) == 2, dim(x) == dim(y))
  for (i in 1:nrow(x))
    for (j in 1:ncol(x))
      x[i, j] <- x[i, j] + y[i, j]
  x
}


naive_relu <- function(x) {
  stopifnot(length(dim(x)) == 2)
  for (i in 1:nrow(x))
    for (j in 1:ncol(x))
      x[i, j] <- max(x[i, j], 0)
  x
}
```


```{r}

system.time({
  for (i in seq_len(1000)) {
    z <- naive_add(x, y)
    z <- naive_relu(z)
  }
})[["elapsed"]]


```



```{r}
X <- random_array(c(64, 3, 32, 10))
y <- random_array(c(10))


```



```{r}
dim(y) <- c(1, 10)
str(y)


```



```{r}
Y <- y[rep(1, 32), ]
str(Y)


```



```{r}
naive_add_matrix_and_vector <- function(x, y) {
  stopifnot(length(dim(x)) == 2,
            length(dim(y)) == 1,
            ncol(x) == dim(y))
  for (i in seq(dim(x)[1]))
    for (j in seq(dim(x)[2]))
      x[i, j] <- x[i, j] + y[j]
  x
}


```



```{r}
x <- random_array(c(32))
y <- random_array(c(32))
z <- x %*% y


```



```{r}
naive_vector_dot <- function(x, y) {
  stopifnot(length(dim(x)) == 1,
            length(dim(y)) == 1,
            dim(x) == dim(y))
  z <- 0
  for (i in seq_along(x))
    z <- z + x[i] * y[i]
  z
}


```



```{r}
naive_matrix_vector_dot <- function(x, y) {
  stopifnot(length(dim(x)) == 2,
            length(dim(y)) == 1,
            nrow(x) == dim(y))
  z <- array(0, dim = nrow(x))
  for (i in 1:nrow(x))
    for (j in 1:ncol(x))
      z[i] <- z[i] + x[i, j] * y[j]
  z
}


```



```{r}
naive_matrix_vector_dot <- function(x, y) {
  z <- array(0, dim = c(nrow(x)))
  for (i in 1:nrow(x))
    z[i] <- naive_vector_dot(x[i, ], y)
  z
}


```



```{r}
naive_matrix_dot <- function(x, y) {
  stopifnot(length(dim(x)) == 2,
            length(dim(y)) == 2,
            ncol(x) == nrow(y))
  z <- array(0, dim = c(nrow(x), ncol(y)))
  for (i in 1:nrow(x))
    for (j in 1:ncol(y)) {
      row_x <- x[i, ]
      column_y <- y[, j]
      z[i, j] <- naive_vector_dot(row_x, column_y)
    }
  z
}


```



```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))


```



```{r}
x <- array(1:6)
x
array_reshape(x, dim = c(3, 2))
array_reshape(x, dim = c(2, 3))


```



```{r}
x <- array(1:6, dim = c(3, 2))
x
t(x)


## ---- eval = FALSE--------------------------------------------------------
## output <- relu(dot(input, W) + b)


## ---- eval = FALSE--------------------------------------------------------
## f(x + epsilon_x) = y + a * epsilon_x


## ---- eval = FALSE--------------------------------------------------------
## y_pred = dot(W, x)
## loss_value = loss_fn(y_pred, y_true)


## ---- eval = FALSE--------------------------------------------------------
## loss_value = f(W)


## ---- eval = FALSE--------------------------------------------------------
## past_velocity <- 0
## momentum <- 0.1
## repeat {
##   p <- get_current_parameters()
##
##   if (p$loss <= 0.01)
##     break
##
##   velocity <- past_velocity * momentum + learning_rate * p$gradient
##   w <- p$w + momentum * velocity - learning_rate * p$gradient
##
##   past_velocity <- velocity
##   update_parameter(w)
## }


## ---- eval = FALSE--------------------------------------------------------
## loss_value <- loss(y_true,
##                    softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))


```



```{r}
fg <- function(x) {
  x1 <- g(x)
  y <- f(x1)
  y
}


## ---- eval = FALSE--------------------------------------------------------
## fghj <- function(x) {
##     x1 <- j(x)
##     x2 <- h(x1)
##     x3 <- g(x2)
##     y <- f(x3)
##     y
## }
##
## grad(y, x) == (grad(y, x3) * grad(x3, x2) * grad(x2, x1) * grad(x1, x))


```



```{r}
library(tensorflow)
x <- tf$Variable(0)
with(tf$GradientTape() %as% tape, {
  y <- 2 * x + 3
})
grad_of_y_wrt_x <- tape$gradient(y, x)


```



```{r}
x <- tf$Variable(array(0, dim = c(2, 2)))
with(tf$GradientTape() %as% tape, {
  y <- 2 * x + 3
})
grad_of_y_wrt_x <- as.array(tape$gradient(y, x))


```



```{r}
W <- tf$Variable(random_array(c(2, 2)))
b <- tf$Variable(array(0, dim = c(2)))

x <- random_array(c(2, 2))
with(tf$GradientTape() %as% tape, {
    y <- tf$matmul(x, W) + b
})
grad_of_y_wrt_W_and_b <- tape$gradient(y, list(W, b))
str(grad_of_y_wrt_W_and_b)


```



```{r}
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255

test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255

train_labels <- mnist$train$y
test_labels <- mnist$test$y


```



```{r}
model <- keras_model_sequential(list(
  layer_dense(units = 512, activation = "relu"),
  layer_dense(units = 10, activation = "softmax")
))


```



```{r}
compile(model,
        optimizer = "rmsprop",
        loss = "sparse_categorical_crossentropy",
        metrics = c("accuracy"))


```



```{r}
fit(model, train_images, train_labels, epochs = 5, batch_size = 128)


## ---- eval = FALSE--------------------------------------------------------
## output <- activation(dot(W, input) + b)


```



```{r}
layer_naive_dense <- function(input_size, output_size, activation) {
  self <- new.env(parent = emptyenv())
  attr(self, "class") <- "NaiveDense"

  self$activation <- activation

  w_shape <- c(input_size, output_size)
  w_initial_value <- random_array(w_shape, min = 0, max = 1e-1)
  self$W <- tf$Variable(w_initial_value)

  b_shape <- c(output_size)
  b_initial_value <- array(0, b_shape)
  self$b <- tf$Variable(b_initial_value)

  self$weights <- list(self$W, self$b)

  self$call <- function(inputs) {
    self$activation(tf$matmul(inputs, self$W) + self$b)
  }

  self
}


```



```{r}
naive_model_sequential <- function(layers) {
  self <- new.env(parent = emptyenv())
  attr(self, "class") <- "NaiveSequential"

  self$layers <- layers

  weights <- lapply(layers, function(layer) layer$weights)
  self$weights <- do.call(c, weights)

  self$call <- function(inputs) {
    x <- inputs
    for (layer in self$layers)
      x <- layer$call(x)
    x
  }

  self
}


```



```{r}
model <- naive_model_sequential(list(
  layer_naive_dense(input_size = 28 * 28, output_size = 512,
                    activation = tf$nn$relu),
  layer_naive_dense(input_size = 512, output_size = 10,
                    activation = tf$nn$softmax)
))
stopifnot(length(model$weights) == 4)


```



```{r}
new_batch_generator <- function(images, labels, batch_size = 128) {
  self <- new.env(parent = emptyenv())
  attr(self, "class") <- "BatchGenerator"
  
  stopifnot(nrow(images) == nrow(labels))
  self$index <- 1
  self$images <- images
  self$labels <- labels
  self$batch_size <- batch_size
  self$num_batches <- ceiling(nrow(images) / batch_size)
  
  self$get_next_batch <- function() {
    start <- self$index
    if (start > nrow(images))
      return(NULL)
    
    end <- start + self$batch_size - 1
    if (end > nrow(images))
      end <- nrow(images)
    
    self$index <- end + 1
    indices <- start:end
    list(images = self$images[indices,],
         labels = self$labels[indices])
  }
  
  self
}


```



```{r}
one_training_step <- function(model, images_batch, labels_batch) {
  with(tf$GradientTape() %as% tape, {
    predictions <- model$call(images_batch)
    per_sample_losses <-
      loss_sparse_categorical_crossentropy(labels_batch, predictions)
    average_loss <- mean(per_sample_losses)
  })
  gradients <- tape$gradient(average_loss, model$weights)
  update_weights(gradients, model$weights)
  average_loss
}


```



```{r}
learning_rate <- 1e-3

update_weights <- function(gradients, weights) {
  stopifnot(length(gradients) == length(weights))
  for (i in seq_along(weights))
    weights[[i]]$assign_sub(gradients[[i]] * learning_rate)
}


```



```{r}
optimizer <- optimizer_sgd(learning_rate = 1e-3)

update_weights <- function(gradients, weights)
  optimizer$apply_gradients(zip_lists(gradients, weights))

```



```{r}
str(zip_lists(gradients = list("grad_for_wt_1", "grad_for_wt_2", "grad_for_wt_3"),
              weights = list("weight_1", "weight_2", "weight_3")))


```



```{r}
fit <- function(model, images, labels, epochs, batch_size = 128) {
  for (epoch_counter in seq_len(epochs)) {
    cat("Epoch ", epoch_counter, "\n")
    batch_generator <- new_batch_generator(images, labels)
    for (batch_counter in seq_len(batch_generator$num_batches)) {
      batch <- batch_generator$get_next_batch()
      loss <- one_training_step(model, batch$images, batch$labels)
      if (batch_counter %% 100 == 0)
        cat(sprintf("loss at batch %s: %.2f\n", batch_counter, loss))
    }
  }
}


```



```{r}
mnist <- dataset_mnist()
train_images <- array_reshape(mnist$train$x, c(60000, 28 * 28)) / 255
test_images <- array_reshape(mnist$test$x, c(10000, 28 * 28)) / 255
test_labels <- mnist$test$y
train_labels <- mnist$train$y

fit(model, train_images, train_labels, epochs = 10, batch_size = 128)


```



```{r}
predictions <- model$call(test_images)
predictions <- as.array(predictions)
predicted_labels <- max.col(predictions) - 1
matches <- predicted_labels == test_labels
cat(sprintf("accuracy: %.2f\n", mean(matches)))
```


#### Links


[1] FranÃ§ois Chollet, Tomasz Kalinowski, and J. J. Allaire, Deep Learning with R, Second Edition, Second Edition. Manning Publications, 2023 [Online]. Available: https://www.manning.com/books/deep-learning-with-r-second-edition. [Accessed: Jun. 08, 2022]

 
