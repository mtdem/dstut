---
title: "CWRU DSCI351-351M-453: Week06b Anscombe's Quartet"
subtitle: "Profs: R. H. French, L. S. Bruckman, P. Leu, K. Davis, S. Cirlos"
author: "TAs: W. Oltjen, K. Hernandez, M. Li, M. Li, D. Colvin" 
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: TRUE
    number_sections: TRUE
    toc_depth: 6
    highlight: tango
  html_notebook:
  html_document:
    css: ../lab.css
    highlight: pygments
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
    df_print: paged
urlcolor: blue
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 6, # the width for plots created by code chunk
  fig.height = 4, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = TRUE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
  # options(tinytex.verbose = TRUE)
```

 \setcounter{section}{6}
 \setcounter{subsection}{2}
 \setcounter{subsubsection}{2}


#### Anscombe’s Quartet of ‘Identical’ Simple Linear Regressions

- Visualization may not be as precise as statistics, 

  - but it provides a unique view onto data 
    - that can make it much easier to discover 
    - interesting structures than numerical methods. 
  - Visualization also provides the context necessary 
    - to make better choices 
    - and to be more careful when fitting models. 

Anscombe’s Quartet is a case in point, 

  - showing that four datasets 
    - that have identical statistical properties (i.e. summary statistics)
    - can indeed be very different.

##### Arguing for Graphics in 1973

- In 1973, Francis J. Anscombe 

  - published a paper titled, **Graphs in Statistical Analysis**.
    - This paper is in 3-readings/2-Articles/
  - The idea of using graphical methods 
    - had been established relatively recently by John Tukey, 
    - but there was evidently still a lot of skepticism. 
  - Anscombe first lists some notions 
    - that textbooks were “indoctrinating” people with, 
    - like the idea that “numerical calculations are exact, 
    - but graphs are rough.”

He then presents a table of numbers. 

  - It contains four distinct datasets (hence the name Anscombe’s Quartet), 
  - each with statistical properties that are essentially identical: 
    - the mean of the x values is 9.0, 
    - mean of y values is 7.5, 
  - they all have nearly identical 
    - variances, 
    - correlations, 
    - and regression lines (to at least two decimal places).

```{r}
# kable is to create tables in LaTeX, HTML, Markdown and reStructuredText
knitr::kable(anscombe)

```

#### Let’s do the simple descriptive statistics on each data set

##### Here is mean of x and y

```{r}
anscombe.1 <- data.frame(x = anscombe[["x1"]], y = anscombe[["y1"]], Set = "Anscombe Set 1")
anscombe.2 <- data.frame(x = anscombe[["x2"]], y = anscombe[["y2"]], Set = "Anscombe Set 2")
anscombe.3 <- data.frame(x = anscombe[["x3"]], y = anscombe[["y3"]], Set = "Anscombe Set 3")
anscombe.4 <- data.frame(x = anscombe[["x4"]], y = anscombe[["y4"]], Set = "Anscombe Set 4")

anscombe.data <- rbind(anscombe.1, anscombe.2, anscombe.3, anscombe.4)
aggregate(cbind(x, y) ~ Set, anscombe.data, mean)
```


#### And SD

```{r}
aggregate(cbind(x, y) ~ Set, anscombe.data, sd)

```

##### And correlation between x and y

```{r}
library(plyr)

correlation <- function(data) {
    x <- data.frame(r = cor(data$x, data$y))
    return(x)
}

ddply(.data = anscombe.data, .variables = "Set", .fun = correlation)
```

As can be seen 

  - they are pretty much the same 
  - for every data set.

#### Let’s perform linear regression model for each

```{r}
model1 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 1"))
model2 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 2"))
model3 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 3"))
model4 <- lm(y ~ x, subset(anscombe.data, Set == "Anscombe Set 4"))
```

##### Here are the summaries

```{r}
summary(model1)
```

```{r}
summary(model2)
```

```{r}
summary(model3)
```

```{r}
summary(model4)
```

#### Now, do what you should have done in the first place: EDA PLOTS

```{r}
library(ggplot2)

ggplot(data = anscombe.data, aes(x = x, y = y)) + 
  geom_point(color = "black") +
  facet_wrap(~Set, ncol = 2) + 
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, data = anscombe.data)

```

##### Review each dataset

- While dataset I 

  - appears like many well-behaved datasets 
    - that have clean and well-fitting linear models, 
  - the others are not served nearly as well. 

Dataset II does not have a linear correlation; 

Dataset III does, 

  - but the linear regression is thrown off by an outlier. 
  - It would be easy to fit a correct linear model, 
    - if only the outlier were spotted 
    - and removed before doing so. 
    
Dataset IV, finally, 

  - does not fit any kind of linear model, 
  - but the single outlier keeps the alarm from going off.

##### How do you find out which model can be applied? 

- Anscombe’s answer is to use graphs: 

  - looking at the data immediately reveals a lot of the structure, 
    - and makes the analyst aware of “pathological” cases like dataset IV. 
  - Computers are not limited to running numerical models, either.

A computer should make both calculations and graphs. 

  - Both sorts of output should be studied; 
  - each will contribute to understanding.

#### What is an Outlier?

- In addition to showing how useful a clear look onto data can be, 

Anscombe also raises an interesting question: 

  - what, exactly, is an outlier? 
  - He describes a study on education, 
    - where he studied per-capita expenditures for public schools 
    - in the 50 U.S. states and the District of Columbia. 
  - Alaska is a bit of an outlier, 
    - so it moves the regression line away from the mainstream. 
  - The obvious response would be to remove Alaska from the data 
    - before computing the regression. 
  - But then, another state will be an outlier. 
  - Where do you stop?

Anscombe argues that the correct answer 

  - is to show both the regression with Alaska, 
  - but also how much it contributes 
    - and what happens when it is removed.

The tool here, again, are graphical representations. 

  - Not only the actual data needs to be shown, 
    - but also the distances from the regression line (the residuals), 
    - and other statistics that help judge how well the model fits. 
  - It seems like an obvious thing to do, 
    - but presumably was not the norm in the 1970s, 
    - and I can imagine that it still not always is.

It can be seen both graphically 

  - and from regression summary 
    - that each data set resulted in same statistical model!
  - Intercepts, 
  - coeficients 
    - and their p values are the same. 
  - SEE (standard error of the estimate, or SD of residuals), 
  - F-value 
    -and it’s p values 
  - are the same.

#### Conclusion: 

- ALWAYS plot your data! 

  - And always do model diagnostics by plotting the residuals.
  
```{r}
par(mfrow = c(2, 2))
plot(model1, main = "Model 1")


plot(model2, main = "Model 2")


plot(model3, main = "Model 3")


plot(model4, main = "Model 4")
```

#### References

[Anscombe, Francis J. (1973) Graphs in statistical analysis. American Statistician, 27, 17–21.](https://amstat.tandfonline.com/doi/abs/10.1080/00031305.1973.10478966?journalCode=utas20)

[What is Anscombe’s Quartet and why is it important? - by Mladen Jovanovic](https://complementarytraining.net/stats-playbook-what-is-anscombes-quartet-and-why-is-it-important/)

[Anscombe’s Quartet - by Robert Kosara](https://eagereyes.org/criticism/anscombes-quartet)

[Anscombe’s Quartet of ‘Identical’ Simple Linear Regressions](https://rstudio-pubs-static.s3.amazonaws.com/52381_36ec82827e4b476fb968d9143aec7c4f.html)





