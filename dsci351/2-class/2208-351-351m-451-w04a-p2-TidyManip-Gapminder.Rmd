---
title: "CWRU DSCI351-351M-451: Week04a Tidy Data Manipulation(CWRU, Pitt, UCF, UTRGV)"
subtitle: "Profs: R. H. French, L. S. Bruckman, P. Leu, K. Davis, S. Cirlos"
author: "TAs: W. Oltjen, K. Hernandez, M. Li, M. Li, D. Colvin" 
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex 
    toc: true
    number_sections: true
    toc_depth: 6
    highlight: tango
  html_document:
    toc: yes
urlcolor: blue
bibliography: refs.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 6, # the width for plots created by code chunk
  fig.height = 4, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = FALSE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
  # options(tinytex.verbose = TRUE)
```

 \setcounter{section}{4}
 \setcounter{subsection}{1}
 \setcounter{subsubsection}{3}
 


#### Tidyverse Cheatsheets, Functions and Reading Your Code

Look at the Tidyverse Cheatsheet 

  - **Tidyverse For Beginners Cheatsheet**
    - In the Git/20s-dsci353-353m-453-prof/3-readings/3-CheatSheets/ folder
  - **Data Wrangling with dplyr and tidyr Cheatsheet**

  
  Tidyverse Functions & Conventions
  
    - The pipe operator `%>%`
    - Use `dplyr::filter()` to subset data row-wise.
    - Use `dplyr::arrange()`  to sort the observations in a data frame
    - Use `dplyr::mutate()` to update or create new columns of a data frame
    - Use `dplyr::summarize()` to turn many observations into a single data point
    - Use `dplyr::arrange()` to change the ordering of the rows of a data frame 
    - Use `dplyr::select()` to choose variables from a tibble, 
      - keeps only variables you mention
    - Use `dplyr::rename()` keeps all the variables and renames variables
      - rename(iris, petal_length = Petal.Length)
    - These can be combined using `dplyr::group_by()` 
      - which lets you perform operations “by group”. 
    - The `%in%` matches conditions provided by a vector using the c() function
    - The **forcats** package has tidyverse functions 
      - for factors (categorical variables)
    - The **readr** package has tidyverse functions 
      - to read_..., melt_... col_..., parse_... data and objects

Reading Your Code: Whenever you see

  - The assignment operator `<-`, think **"gets"**
  - The pipe operator, `%>%`, think **"then"**


#### What is a Tidy Data Frame

```{r}
library(devtools)
# devtools::install_github("rstudio/EDAWR")
```

##### What is data wrangling? Intro, Motivation, Outline, Setup

  - [Pt. 1 Data Wrangling Introduction](https://www.youtube.com/watch?v=jOd65mR1zfw&list=WL&index=6&t=14s)
    - [Tibbles](https://youtu.be/jOd65mR1zfw?list=WL&t=141)
    - [View](https://youtu.be/jOd65mR1zfw?list=WL&t=298)
    - [Pipe Operator](https://youtu.be/jOd65mR1zfw?list=WL&t=327)
  - [Pt 2 Intro to Data Wrangling with R and the Tidyverse](https://youtu.be/1ELALQlO-yM?list=WL)
    - [What is a Tidy Dataframe?](https://youtu.be/1ELALQlO-yM?list=WL&t=99)
    - [tidyr package for gather and spread](https://youtu.be/1ELALQlO-yM?list=WL&t=462)
  - [dplyr -- Pt 3 Intro to the Grammar of Data Manipulation with R](https://youtu.be/Zc_ufg4uW4U?list=WL)
  - [Working with Two Datasets: Binds, Set Operations, and Joins 
  - Pt 4 Intro to Data Manipulation](https://youtu.be/AuBgYDCg1Cg?list=WL)

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
set.seed(4561)
```

##### Buckle your seat belt

- *Ignore if you don't need this bit of support.*

Now is the time to make sure 

  - you are working in an appropriate directory on your computer, 
  - probably through the use of an [RStudio Project](block002_hello-r-workspace-wd-project.html). 

To see where you are

  - Enter `getwd()` in the Console to see current working directory or, 
  - in RStudio, this is displayed in the bar at the top of Console.

You should clean out your work space. 

  - In RStudio, click on the "Clear" broom icon from the Environment tab or 
  - use *Session > Clear Work space*. 
  - You can also enter `rm(list = ls())` in the Console to accomplish same.

Now restart R. 

  - This will ensure you don't have any packages loaded 
    - from previous calls to `library()`. 
  - In RStudio, use *Session > Restart R*. 
  - Otherwise, quit R with `q()` and re-launch it.

Why do we do this? So that the code you write is complete and re-runnable. 

  - If you return to a clean slate often, 
    - you will root out hidden dependencies 
    - where one snippet of code only works 
    - because it relies on objects created by code saved elsewhere 
    - or, much worse, never saved at all. 
  - Similarly, an aggressive clean slate approach 
    - will expose any usage of packages 
    - that have not been explicitly loaded. 

Finally, open a new R script 

  - and develop and run your code from there. 
  - In RStudio, use *File > New File > R Script*. 
    - Save this script with a name ending in `.r` or `.R`, 
    - containing no spaces or other funny stuff, 
    - and that evokes whatever it is we're doing today. 
  - Example: `cm004_data-care-feeding.r`.

Another great idea is to do this in an R Markdown document. 

##### Data frames are awesome

- Whenever you have rectangular, spreadsheet-y data, 

  - your default data receptacle in R is a data frame. 
  - Do not depart from this without good reason. 
  
Data frames are awesome because...

  - Data frames package related variables neatly together,
    - keeping them in sync vis-a-vis row order
    - applying any filtering of observations uniformly.
  - Most functions for inference, modeling, and graphing 
    - are happy to be passed a data frame via a `data =` argument. 
    - This has been true in base R for a long time.
  - The set of packages known as the [`tidyverse`](https://github.com/hadley/tidyverse)
    - takes this one step further 
    - and explicitly prioritizes the processing of data frames. 
  - This includes popular packages like `dplyr` and `ggplot2`. 
  - In fact the tidyverse prioritizes 
    - a special flavor of data frame, called a "tibble."

Data frames 

  - unlike general arrays or, specifically, matrices in R 
  - can hold variables of different flavors, 
    - such as character data (subject ID or name), 
    - quantitative data (white blood cell count), 
    - and categorical information (treated vs. untreated). 
  - If you use homogeneous structures, 
    - like matrices, 
    - for data analysis, 
    - you  are likely to make the terrible mistake 
    - of spreading a data set out over multiple, unlinked objects. 
  - Why? Because you can't put character data, 
    - such as subject name, 
    - into the numeric matrix that holds white blood cell count. 
  - This fragmentation is a Bad Idea.

#### Get the Gapminder data

- What is Gapminder

  * A project of Hans Rosling
  * [Gapminder Project](https://www.gapminder.org/tools/#_chart-type=bubbles)

[Hans Rosling and Gapminder: 200 years in 4 minutes - BBC News](https://www.youtube.com/watch?v=Z8t4k0Q8e8Y)

We will work with some of the data from the [Gapminder project](http://www.gapminder.org). 

This is released as an R package, 

  - so we can install it from CRAN like so:

```{r eval = FALSE}
# install.packages("gapminder")
```

Now load the package:

```{r}
library(gapminder)
```

    ##### Meet the `gapminder` data frame or "tibble"

By loading the `gapminder` package, 

  - we now have access to a data frame by the same name. 

Get an overview of this with `str()`, 

  - which displays the structure of an object.

```{r}
str(gapminder)
```

`str()` will provide a sensible description of almost anything 

  - and, worst case, nothing bad can actually happen. 
  - When in doubt, just `str()` some of the recently created objects 
    - to get some ideas about what to do next.

We could print the `gapminder` object itself to screen. 

  - However, if you've used R before, you might be reluctant to do this, 
  - because large data sets just fill up your Console 
    - and provide very little insight.

This is the first big win for **tibbles**. 

  - The [`tidyverse`](https://github.com/hadley/tidyverse) 
  - offers a special case of R's default data frame: the "tibble", 
    - which is a nod to the actual class of these objects, `tbl_df`.

If you have not already done so, 

  - install the `tidyverse` meta-package now:

```{r eval = FALSE}
# install.packages("tidyverse")
```

Now load it:

```{r}
library(tidyverse)
```

Now we can boldly print `gapminder` to screen! 

  - It is a tibble (and also a regular data frame) 
  - and the `tidyverse` provides a nice print method 
    - that shows the most important stuff 
    - and doesn't fill up your Console.

```{r}
## see? it's still a regular data frame, but also a tibble
class(gapminder)
gapminder
```

If you are dealing with plain vanilla data frames, 

  - you can rein in data frame printing explicitly 
    - with `head()` and `tail()`. 
  - Or turn it into a tibble with `as_tibble()`!

```{r}
head(gapminder)
tail(gapminder)
as_tibble(iris)
```

More ways to query basic info on a data frame:

```{r}
names(gapminder)
ncol(gapminder)
length(gapminder)
dim(gapminder)
nrow(gapminder)
```

A statistical overview can be obtained with `summary()`

```{r}
summary(gapminder)
```

Although we haven't begun our formal coverage of visualization yet, 

  - it's so important for smell-testing data set 
    - that we will make a few figures anyway. 
  - Here we use only base R graphics, which are very basic.

```{r first-plots-base-R}
plot(lifeExp ~ year, gapminder)
plot(lifeExp ~ gdpPercap, gapminder)
plot(lifeExp ~ log(gdpPercap), gapminder)
```

#### Non-sequitur: The Equals Operator

- Sidebar on equals: 

  - A single equal sign `=` is most commonly used 
    - to specify values of arguments when calling functions in R, 
    - e.g. `group = continent`. 
  - It can be used for assignment 
    - but we advise against that, 
    - in favor of `<-`. 
  - A double equal sign `==` is a binary comparison operator, 
    - akin to less than `<` or greater than `>`, 
    - returning the logical value `TRUE` in the case of equality 
    - and `FALSE` otherwise. 
  - Although you may not yet understand exactly why, 
    - `subset = country == "Colombia"` restricts operation -- scatter plotting, 
    - in above examples -- to observations where the country is Colombia.


Let's go back to the result of `str()` to talk about what a data frame is.

```{r}
str(gapminder)
```

A data frame is a special case of a *list*, 

  - which is used in R to hold just about anything. 
  
Data frames are a special case 

  - where the length of each list component is the same. 
  
Data frames are superior to matrices in R 

  - because they can hold vectors of different flavors, 
  - e.g. numeric, character, and categorical data can be stored together. 
  - This comes up a lot!

####### Look at the variables inside a data frame

- To specify a single variable from a data frame, 

  - use the dollar sign `$`. 
  
Let's explore the numeric variable for life expectancy.

```{r histogram-lifeExp}
head(gapminder$lifeExp)
summary(gapminder$lifeExp)
hist(gapminder$lifeExp)
```

The year variable is an integer variable, 

  - but since there are so few unique values 
  - it also functions a bit like a categorical variable.

```{r}
summary(gapminder$year)
table(gapminder$year)
```

The variables for country and continent 

  - hold truly categorical information, 
  - which is stored as a *factor* in R.
  
```{r}
class(gapminder$continent)
summary(gapminder$continent)
levels(gapminder$continent)
nlevels(gapminder$continent)
```

The __levels__ of the factor `continent` 

  - are "Africa", "Americas", etc. 
  - and this is what's usually presented to your eyeballs by R. 

In general, the levels are friendly human-readable character strings, 

  - like "male/female" and "control/treated". 
  - But *never ever ever* forget that, under the hood, 
    - R is really storing integer codes 1, 2, 3, etc. 
  - Look at the result from `str(gapminder$continent)` 
    - if you are skeptical.

```{r}
str(gapminder$continent)
```

This [Janus](http://en.wikipedia.org/wiki/Janus)-like nature of factors 

  - means they are rich with booby traps for the unsuspecting 
  - but they are a necessary evil. 
  
I recommend you resolve to learn how to properly care and feed your *factors* 

 - The pros far outweigh the cons. 
 
Specifically in modeling and figure-making, 

  - factors are anticipated and accommodated 
  - by the functions and packages you will want to exploit.

Here we count how many observations are associated with each continent 

  - and, as usual, try to portray that info visually. 
  
This makes it much easier to quickly see 

  - that African countries are well represented in this data set.

```{r tabulate-continent}
table(gapminder$continent)
barplot(table(gapminder$continent))
```

In the figures below, we see how factors

  - can be put to work in figures. 
  
The `continent` factor is easily mapped 

  - into "facets" or colors and a legend 
    - by the `ggplot2` package. 
  - *Making figures with `ggplot2` is covered elsewhere 
    - so feel free to just sit back and enjoy these plots
    - or blindly copy/paste.*

```{r factors-nice-for-plots, fig.show = 'hold', out.width = '49%'}
## we exploit the fact that ggplot2 was installed and loaded via the tidyverse
p <- ggplot(filter(gapminder, continent != "Oceania"),
            aes(x = gdpPercap, y = lifeExp)) # just initializes
p <- p + scale_x_log10() # log the x axis the right way
p + geom_point() # scatterplot
p + geom_point(aes(color = continent)) # map continent to color
p + geom_point(alpha = (1 / 3), size = 3) + geom_smooth(lwd = 3, se = FALSE)
p + geom_point(alpha = (1 / 3), size = 3) + facet_wrap( ~ continent) +
  geom_smooth(lwd = 1.5, se = FALSE)
```


####### Recap

  - Use data frames!!!
  - Use the [`tidyverse`](https://github.com/hadley/tidyverse)!!! This will provide a special type of data frame called a "tibble" that has nice default printing behavior, among other benefits.
  - When in doubt, `str()` something or print something.
  - Always understand the basic extent of your data frames: number of rows and columns.
  - Understand what flavor the variables are.
  - Use factors!!! But with intention and care.
  - Do basic statistical and visual sanity checking of each variable.
  - Refer to variables by name, e.g., `gapminder$lifeExp`, not by column number. Your code will be more robust and readable.

CLASS STOPPED HERE

#### Tidy Manipulation: Introduction to dplyr package

##### Intro

- `dplyr` is a package for data manipulation, 

  - developed by Hadley Wickham and Romain Francois. 
  - It is built to be fast, highly expressive, and open-minded 
    -  about how your data is stored. 
  - It is installed as part of the the [`tidyverse`](https://github.com/hadley/tidyverse) meta-package 
    - and, as a core package, it is among those loaded via `library(tidyverse)`.

`dplyr`'s roots are in an earlier package called [`plyr`](http://plyr.had.co.nz), 

  - which implements the ["split-apply-combine" strategy for data analysis](https://www.jstatsoft.org/article/view/v040i01) (PDF). 
  - Where `plyr` covers a diverse set of inputs and outputs 
    - (e.g., arrays, data frames, lists), 
  - `dplyr` has a laser-like focus on data frames 
    - or, in the `tidyverse`, "tibbles". 
  - `dplyr` is a package-level treatment of the `ddply()` function 
    - from `plyr`, 
  - because "data frame in, data frame out" 
  - proved to be so incredibly important.

Have no idea what I'm talking about? Not sure if you care? 

  - If you use these base R functions: 
    - `subset()`, `apply()`, `[sl]apply()`, `tapply()`, `aggregate()`, 
    - `split()`, `do.call()`, `with()`, `within()`, 
    - then you should keep reading. 
  - Also, if you use `for()` loops a lot, 
    - you might enjoy learning other ways 
    - to iterate over rows or groups of rows 
    - or variables in a data frame.

###### Load `dplyr` and `gapminder`

I choose to load the `tidyverse`, 

  - which will load `dplyr`, 
    - among other packages we use incidentally below. 

Also load `gapminder`.

```{r}
# library(gapminder)
# library(tidyverse)
```

###### Say hello to the Gapminder tibble

- The `gapminder` data frame is a special kind of data frame: a tibble.

```{r}
gapminder
```

It's tibble-ness is why we get nice compact printing. 

  - For a reminder of the problems with base data frame printing, 
    - go type `iris` in the R Console 
  - or, better yet, print a data frame to screen 
    - that has lots of columns.

Note how gapminder's `class()` includes `tbl_df`; 

  - the "tibble" terminology is a nod to this.

```{r}
class(gapminder)
```

There will be some functions, like `print()`, 

  - that know about tibbles and do something special. 
  - There will others that do not, like `summary()`. 
  - In which case the regular data frame treatment will happen, 
    - because every tibble is also a regular data frame.

To turn any data frame into a tibble use `as_tibble()`:

```{r}
as_tibble(iris)
```

##### Think before you create excerpts of your data ...

- If you feel the urge to store a little snippet of your data:

```{r}
(canada <- gapminder[241:252, ])
```

Stop and ask yourself ...

> Do I want to create mini data sets for each level of some factor (or unique combination of several factors) ... in order to compute or graph something?  

> If YES, __use proper data aggregation techniques__ or faceting in `ggplot2` -- __don’t subset the data__. Or, more realistic, only subset the data as a temporary measure while you develop your elegant code for computing on or visualizing these data subsets.

> If NO, then maybe you really do need to store a copy of a subset of the data. But seriously consider whether you can achieve your goals by simply using the `subset =` argument of, e.g., the `lm()` function, to limit computation to your excerpt of choice. Lots of functions offer a `subset =` argument!

Copies and excerpts of your data 

  - clutter your work space, 
    - invite mistakes, 
    - and sow general confusion. 
  - Avoid whenever possible.

Reality can also lie somewhere in between. 

  - You will find the workflows presented below 
    - can help you accomplish your goals 
  - with minimal creation of temporary, intermediate objects.

##### Use `filter()` to subset data row-wise.

- `filter()` takes logical expressions 

  - and returns the rows for which all are `TRUE`.

```{r}
filter(gapminder, lifeExp < 29)
filter(gapminder, country == "Rwanda", year > 1979)
filter(gapminder, country %in% c("Rwanda", "Afghanistan"))
```

Compare with some base R code to accomplish the same things

```{r eval = FALSE}
gapminder[gapminder$lifeExp < 29,] ## repeat `gapminder`, [i, j] indexing is distracting
subset(gapminder, country == "Rwanda") ## almost same as filter; quite nice actually
```

Under no circumstances 

  - should you subset your data 
    - the way I did at first:

```{r eval = FALSE}
excerpt <- gapminder[241:252, ]
```

Why is this a terrible idea?

  * It is not self-documenting. 
    - What is so special about rows 241 through 252?
  * It is fragile. 
    - This line of code will produce different results 
    - if someone changes the row order of `gapminder`, 
    - e.g. sorts the data earlier in the script.
  
```{r eval = FALSE}
filter(gapminder, country == "Canada")
```

This call explains itself and is fairly robust.

##### Meet the new pipe operator

- Before we go any further, 

  - we should exploit the new pipe operator 
  - that the tidyverse imports 
    - from the [`magrittr`](https://github.com/smbache/magrittr) package by Stefan Bache. 

This is going to change your data analytic life. 

  - You no longer need to enact multi-operation commands 
    - by nesting them inside each other, 
    - like so many Russian nesting dolls. 
  - This new syntax leads to code 
    - that is much easier to write and to read.

Here's what it looks like: `%>%`. 

  - The RStudio keyboard shortcut: 
    - Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac).

Let's demo then I'll explain:

```{r}
gapminder %>% head()
```

This is equivalent to `head(gapminder)`. 

  - The pipe operator takes the thing on the left-hand-side 
    - and __pipes__ it into the function call 
    - on the right-hand-side 
  - literally, drops it in as the first argument.

Never fear, you can still specify other arguments to this function! 

To see the first 3 rows of Gapminder, 

  - we could say `head(gapminder, 3)` or this:

```{r}
gapminder %>% head(3)
```

I've advised you to think 

  - "gets" whenever you see the assignment operator, `<-`. 
  
Similarly, you should think 

  - "then" whenever you see the pipe operator, `%>%`.

You are probably not impressed yet, but the magic will soon happen.

##### Use `select()` to subset the data on variables or columns.

Back to `dplyr` ...

Use `select()` to subset the data on variables or columns. Here's a conventional call:

```{r}
select(gapminder, year, lifeExp)
```

And here's the same operation, 

  - but written with the pipe operator 
  - and piped through `head()`:

```{r}
gapminder %>%
  select(year, lifeExp) %>%
  head(4)
```

Think: "Take `gapminder`, 

  - then select the variables year and lifeExp, 
  - then show the first 4 rows."

##### Revel in the convenience

- Here's the data for Cambodia, 

  - but only certain variables:

```{r}
gapminder %>%
  filter(country == "Cambodia") %>%
  select(year, lifeExp)
```

and what a typical base R call would look like:

```{r}
gapminder[gapminder$country == "Cambodia", c("year", "lifeExp")]
```

##### Pure, predictable, pipe able

- We've barely scratched the surface of `dplyr` 

  - but I want to point out key principles you may start to appreciate. 
  - If you're new to R or "programming with data", 
    - feel free skip this section 
    - and [move on](block010_dplyr-end-single-table.html).

`dplyr`'s verbs, such as `filter()` and `select()`, 

  - are what's called [pure functions](http://en.wikipedia.org/wiki/Pure_function). 
  - To quote from Wickham's [Advanced R Programming book](http://adv-r.had.co.nz/Functions.html):

> The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the work space. 

> In other words, pure functions have no side effects: they don’t affect the state of the world in any way apart from the value they return.

> In fact, these verbs are a special case of pure functions: they take the same flavor of object as input and output. 

> Namely, a data frame or one of the other data receptacles `dplyr` supports.

And finally, 

  - the data is __always__ 
    - the very first argument of the verb functions.

This set of deliberate design choices, 

  - together with the new pipe operator, 
  - produces a highly effective, 
    - low friction [domain-specific language](http://adv-r.had.co.nz/dsl.html) 
    - for data analysis.

Go to the next block, [`dplyr` functions for a single dataset](block010_dplyr-end-single-table.html), for more `dplyr`!

##### Resources

- `dplyr` official stuff

  * package home [on CRAN](http://cran.r-project.org/web/packages/dplyr/index.html)
    - note there are several vignettes, with the [introduction](http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html) being the most relevant right now
    - the [one on window functions](http://cran.rstudio.com/web/packages/dplyr/vignettes/window-functions.html) will also be interesting to you now
  * development home [on GitHub](https://github.com/hadley/dplyr)
  

[RStudio Data Wrangling cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), covering `dplyr` and `tidyr`. Remember you can get to these via *Help > Cheat sheets.* 

[Excellent slides](https://github.com/tjmahr/MadR_Pipelines) on pipelines and `dplyr` by TJ Mahr, talk given to the Madison R Users Group.

Blog post [Hands-on dplyr tutorial for faster data manipulation in R](http://www.dataschool.io/dplyr-tutorial-for-faster-data-manipulation-in-r/) by Data School, that includes a link to an R Markdown document and links to videos


###### dplyr functions for a single data set

- In the introduction to dplyr, we used two very important verbs and an operator:

  - `filter()` for subsetting data with row logic
  - `select()` for subsetting data variable- or column-wise
  - the pipe operator `%>%`, 
    - which feeds the LHS as the first argument 
    - to the expression on the RHS
  
We also discussed dplyr's role inside the tidyverse and tibbles:

  - dplyr is a core package in the [tidyverse](https://github.com/hadley/tidyverse) meta-package. 
  - Since we often make incidental usage of the others, 
    - we will load dplyr and the others via `library(tidyverse)`.
  - The tidyverse embraces a special flavor of data frame, 
    - called a tibble. 
  - The `gapminder` data set is stored as a tibble.  

##### Load dplyr and gapminder

- I choose to load the tidyverse, which will load dplyr, among other packages we use incidentally below. Also load gapminder.

```{r}
# library(gapminder)
# library(tidyverse)
```

##### Create a copy of gapminder

- We're going to make changes to the `gapminder` tibble. 

To eliminate any fear 

  - that you're damaging the data that comes with the package, 
  - we create an explicit copy of `gapminder` for our experiments.

```{r}
(my_gap <- gapminder)
```

**Pay close attention** to when we evaluate statements 

  - but let the output just print to screen:

```{r eval = FALSE}
## let output print to screen, but do not store
my_gap %>% filter(country == "Canada")
```

... versus when we assign the output to an object, 

  - possibly overwriting an existing object.

```{r eval = FALSE}
## store the output as an R object
my_precious <- my_gap %>% filter(country == "Canada")
```

##### Use `mutate()` to add new variables

- Imagine we wanted to recover each country's GDP. 

After all, the Gapminder data 

  - has a variable for population 
    - and GDP per capita. 
  - Let's multiply them together.

`mutate()` is a function that 

  - defines and inserts new variables into a tibble. 
  - You can refer to existing variables by name.

```{r}
my_gap %>%
  mutate(gdp = pop * gdpPercap)
```

Hmmmm ... those GDP numbers are almost uselessly large and abstract. 

Consider the advice of Randall Munroe of xkcd:

>One thing that bothers me is large numbers presented without context... 

> 'If I added a zero to this number, would the sentence containing it mean something different to me?' 

> If the answer is 'no,' maybe the number has no business being in the sentence in the first place."

> Maybe it would be more meaningful to consumers of my tables and figures to stick with GDP per capita. 

> But what if I reported GDP per capita, *relative to some benchmark country*. 

> Since Canada is my adopted home, I'll go with that. 

I need to create a new variable 

  - that is `gdpPercap` divided by Canadian `gdpPercap`, 
    - taking care that I always divide two numbers that pertain to the same year.

How I achieve:

  - Filter down to the rows for Canada.
  - Create a new temporary variable in `my_gap`:
    - Extract the `gdpPercap` variable from the Canadian data.
    - Replicate it once per country in the data set, so it has the right length.
  - Divide raw `gdpPercap` by this Canadian figure.
  - Discard the temporary variable of replicated Canadian `gdpPercap`.

```{r}
ctib <- my_gap %>%
  filter(country == "Canada")
## this is a semi-dangerous way to add this variable
## I'd prefer to join on year, but we haven't covered joins yet
my_gap <- my_gap %>%
  mutate(
    tmp = rep(ctib$gdpPercap, nlevels(country)),
    gdpPercapRel = gdpPercap / tmp,
    tmp = NULL
  )
```

Note that, `mutate()` builds new variables sequentially 

  - so you can reference earlier ones (like `tmp`) 
    - when defining later ones (like `gdpPercapRel`). 
  - Also, you can get rid of a variable 
    - by setting it to `NULL`.

How could we sanity check that this worked? 

  - The Canadian values for `gdpPercapRel` better all be 1!

```{r}
my_gap %>%
  filter(country == "Canada") %>%
  select(country, year, gdpPercapRel)
```

I perceive Canada to be a "high GDP" country, 

  - so I predict that the distribution of `gdpPercapRel` is located below 1, 
    - possibly even well below. 
  - Check your intuition!

```{r}
summary(my_gap$gdpPercapRel)
```

The relative GDP per capita numbers are, in general, well below 1. 

We see that most of the countries covered by this data set 

  - have substantially lower GDP per capita, relative to Canada, 
  - across the entire time period.

Remember: Trust No One. Including (especially?) yourself. 

  - Always try to find a way to check that you've done what meant to. 
  - Prepare to be horrified.

##### Use `arrange()` to row-order data in a principled way

- `arrange()` reorders the rows in a data frame. 

  - Imagine you wanted this data ordered by year then country, 
    - as opposed to by country then year.

```{r}
my_gap %>%
  arrange(year, country)
```

Or maybe you want just the data from 2007, 

  - sorted on life expectancy?

```{r}
my_gap %>%
  filter(year == 2007) %>%
  arrange(lifeExp)
```

Oh, you'd like to sort on life expectancy 

  - in **desc**ending order? Then use `desc()`.

```{r}
my_gap %>%
  filter(year == 2007) %>%
  arrange(desc(lifeExp))
```

I advise that your analyses 

  - NEVER rely on rows or variables being in a specific order. 
  - But it's still true that human beings write the code 
    - and the interactive development process can be much nicer 
    - if you reorder the rows of your data as you go along. 
  - Also, once you are preparing tables for human eyeballs, 
    - it is imperative that you step up 
    - and take control of row order.

##### Use `rename()` to rename variables

- When I first cleaned this Gapminder excerpt, 

  - I was a [`camelCase`](http://en.wikipedia.org/wiki/CamelCase) person, 
  - but now I'm all about [`snake_case`](http://en.wikipedia.org/wiki/Snake_case). 

So I am vexed by the variable names I chose 

  - when I cleaned this data years ago. 
  - Let's rename some variables!

```{r}
my_gap %>%
  rename(life_exp = lifeExp,
         gdp_percap = gdpPercap,
         gdp_percap_rel = gdpPercapRel)
```

I did NOT assign the post-rename object back to `my_gap` 

  - because that would make the chunks in this practicum 
    - harder to copy/paste and run out of order. 
  - In real life, I would probably assign this back to `my_gap`, 
    - in a data preparation script, 
    - and proceed with the new variable names.

##### `select()` can rename and reposition variables

- You've seen simple use of `select()`. 

There are two tricks you might enjoy:

  1. `select()` can rename the variables you request to keep.
  2. `select()` can be used with `everything()` to hoist a variable up to the front of the tibble.
  
```{r}
my_gap %>%
  filter(country == "Burundi", year > 1996) %>%
  select(yr = year, lifeExp, gdpPercap) %>%
  select(gdpPercap, everything())
```

`everything()` is one of several helpers for variable selection. 

  - Read its help to see the rest.

##### `group_by()` is a mighty weapon

- I have found ~~friends and family~~ collaborators 

  - love to ask seemingly innocuous questions like, 
    - "which country experienced the sharpest 5-year drop in life expectancy?". 
  - In fact, that is a totally natural question to ask. 
    - But if you are using a language that doesn't know about data, 
    - it's an incredibly annoying question to answer.

dplyr offers powerful tools to solve this class of problem.

  * `group_by()` adds extra structure to your data set -- grouping information -- which lays the groundwork for computations within the groups.
  * `summarize()` takes a data set with $n$ observations, computes requested summaries, and returns a data set with 1 observation.
  * Window functions take a data set with $n$ observations and return a data set with $n$ observations.
  * `mutate()` and `summarize()` will honor groups.
  * You can also do very general computations on your groups with `do()`, though elsewhere in this course, I advocate for other approaches that I find more intuitive, using the `purrr` package.
  
Combined with the verbs you already know, 

  - these new tools allow you 
    - to solve an extremely diverse set of problems 
    - with relative ease.

###### Counting things up

- Let's start with simple counting.  

How many observations do we have per continent?

```{r}
my_gap %>%
  group_by(continent) %>%
  summarize(n = n())
```

Let us pause here to think about the tidyverse. 

You could get these same frequencies using `table()` from base R.

```{r}
table(gapminder$continent)
str(table(gapminder$continent))
```

But the object of class `table` that is returned 

  - makes downstream computation a bit fiddlier than you'd like. 
  
For example, it's too bad the continent levels 

  - come back only as *names* 
    - and not as a proper factor, 
    - with the original set of levels. 

This is an example of how the tidyverse 

  - smooths transitions where you want 
  - the output of step i 
    - to become the input of step i + 1.

The `tally()` function is a convenience function 

  - that knows to count rows. 
  - It honors groups.

```{r}
my_gap %>%
  group_by(continent) %>%
  tally()
```

The `count()` function is an even more convenient function 

  - that does both grouping and counting.

```{r}
my_gap %>% 
  count(continent)
```

What if we wanted to add the number of unique countries for each continent? 

You can compute multiple summaries inside `summarize()`. 

  - Use the `n_distinct()` function 
    - to count the number of distinct countries 
    - within each continent.

```{r}
my_gap %>%
  group_by(continent) %>%
  summarize(n = n(),
            n_countries = n_distinct(country))
```

###### General summarization

- The functions you'll apply within `summarize()` 

  - include classical statistical summaries, 
    - like  `mean()`, `median()`, `var()`, `sd()`, `mad()`, 
    - `IQR()`, `min()`, and `max()`. 
  - Remember they are functions that take $n$ inputs 
    - and distill them down into 1 output.

Although this may be statistically ill-advised, 

  - let's compute the average life expectancy by continent.

```{r}
my_gap %>%
  group_by(continent) %>%
  summarize(avg_lifeExp = mean(lifeExp))
```

`summarize_at()` applies the same summary function(s) 

  - to multiple variables. 
  - Let's compute average and median life expectancy and GDP per capita 
    - by continent by year ... 
    - but only for 1952 and 2007.

```{r}
my_gap %>%
  filter(year %in% c(1952, 2007)) %>%
  group_by(continent, year) %>%
  summarize_at(vars(lifeExp, gdpPercap), funs(mean, median))
```

Let's focus just on Asia. 

  - What are the minimum and maximum life expectancies 
  - seen by year?

```{r}
my_gap %>%
  filter(continent == "Asia") %>%
  group_by(year) %>%
  summarize(min_lifeExp = min(lifeExp),
            max_lifeExp = max(lifeExp))
```

Of course it would be much more interesting to see 

  - *which* country contributed these extreme observations. 
    - Is the minimum (maximum) always coming from the same country? 
  - We tackle that with window functions shortly.

##### Grouped mutate

- Sometimes you don't want to collapse the $n$ rows for each group into one row. 

  - You want to keep your groups, 
  - but compute within them.

###### Computing with group-wise summaries

- Let's make a new variable that is 

  - the years of life expectancy gained (lost) relative to 1952, 
    - for each individual country. 
  - We group by country 
    - and use `mutate()` to make a new variable. 
  - The `first()` function extracts the first value from a vector. 
  - Notice that `first()` is 
    - operating on the vector of life expectancies 
    - *within each country group*.

```{r}
my_gap %>%
  group_by(country) %>%
  select(country, year, lifeExp) %>%
  mutate(lifeExp_gain = lifeExp - first(lifeExp)) %>%
  filter(year < 1963)
```

Within country, 

  - we take the difference between life expectancy in year $i$ 
    - and life expectancy in 1952. 
  - Therefore we always see zeroes for 1952 and, 
    - for most countries, 
    - a sequence of positive and increasing numbers.

###### Window functions

- Window functions 

  - take $n$ inputs 
    - and give back $n$ outputs. 
  - Furthermore, the output depends on all the values. 
  - So `rank()` is a window function 
    - but `log()` is not. 

Here we use window functions 

  - based on ranks and offsets.

Let's revisit the worst and best life expectancies in Asia over time,

  - but retaining info about *which* country 
  - contributes these extreme values.

```{r}
my_gap %>%
  filter(continent == "Asia") %>%
  select(year, country, lifeExp) %>%
  group_by(year) %>%
  filter(min_rank(desc(lifeExp)) < 2 | min_rank(lifeExp) < 2) %>%
  arrange(year) %>%
  print(n = Inf)
```

We see that (min = Afghanistan, max = Japan) is the most frequent result, 

  - but Cambodia and Israel pop up at least once each 
    - as the min or max, respectively. 
  - That table should make you impatient for our upcoming work 
    - on tidying and reshaping data!
    
Wouldn't it be nice to have one row per year?

  - How did that actually work? 
  - First, I store and view a partial 
    - that leaves off the `filter()` statement. 
  - All of these operations should be familiar.

```{r}
asia <- my_gap %>%
  filter(continent == "Asia") %>%
  select(year, country, lifeExp) %>%
  group_by(year)
asia
```

Now we apply a window function -- `min_rank()`. 

  - Since `asia` is grouped by year, 
    - `min_rank()` operates within mini-data sets, 
    - each for a specific year. 
  - Applied to the variable `lifeExp`, `min_rank()` 
    - returns the rank of each country's observed life expectancy. 
  - FYI, the `min` part just specifies how ties are broken. 
  - Here is an explicit peek at these within-year life expectancy ranks, 
    - in both the (default) ascending and descending order.

For concreteness, I use `mutate()` 

  - to actually create these variables, 
    - even though I dropped this in the solution above. 
  - Let's look at a bit of that.

```{r}
asia %>%
  mutate(le_rank = min_rank(lifeExp),
         le_desc_rank = min_rank(desc(lifeExp))) %>%
  filter(country %in% c("Afghanistan", "Japan", "Thailand"), year > 1995)
```

Afghanistan tends to present 1's in the `le_rank` variable, 

  - Japan tends to present 1's in the `le_desc_rank` variable 
  - and other countries, 
    - like Thailand, 
    - present less extreme ranks.

You can understand the original `filter()` statement now:

```{r eval = FALSE}
# filter(min_rank(desc(asia$lifeExp)) < 2 | min_rank(asia$lifeExp) < 2)
```

These two sets of ranks are formed on-the-fly, within year group, 

  - and `filter()` retains rows with rank less than 2, 
    - which means ... the row with rank = 1. 
  - Since we do for ascending and descending ranks, 
    - we get both the min and the max.
  - If we had wanted just the min OR the max, 
    - an alternative approach using `top_n()` 
    - would have worked.

```{r}
my_gap %>%
  filter(continent == "Asia") %>%
  select(year, country, lifeExp) %>%
  arrange(year) %>%
  group_by(year) %>%
  #top_n(1, wt = lifeExp)        ## gets the min
  top_n(1, wt = desc(lifeExp)) ## gets the max
```

##### Grand Finale

- So let's answer that "simple" question: 

  - which country experienced the sharpest 5-year drop in life expectancy? 
  - Recall that this excerpt of the Gapminder data 
    - only has data every five years, e.g. for 1952, 1957, etc. 
  - So this really means looking at life expectancy changes 
    - between adjacent time points.
  - At this point, that's just too easy, 
    - so let's do it by continent while we're at it.

```{r}
my_gap %>%
  select(country, year, continent, lifeExp) %>%
  group_by(continent, country) %>%
  ## within country, take (lifeExp in year i) - (lifeExp in year i - 1)
  ## positive means lifeExp went up, negative means it went down
  mutate(le_delta = lifeExp - lag(lifeExp)) %>%
  ## within country, retain the worst lifeExp change = smallest or most negative
  summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) %>%
  ## within continent, retain the row with the lowest worst_le_delta
  top_n(-1, wt = worst_le_delta) %>%
  arrange(worst_le_delta)
```

Ponder that for a while. 

The subject matter and the code. 

Mostly you're seeing what genocide looks like 

  - in dry statistics 
  - on average life expectancy.

Break the code into pieces, 

  - starting at the top, 
  - and inspect the intermediate results. 
  
That's certainly how I was able to *write* such a thing. 

These commands do not leap fully formed 

  - out of anyone's forehead 
  - they are built up gradually, 
    - with lots of errors and refinements along the way. 
  - I'm not even sure it's a great idea 
    - to do so much manipulation in one fell swoop. 

Is the statement above really hard for you to read? 

  - If yes, then by all means 
    - break it into pieces 
    - and make some intermediate objects. 
  - Your code should be easy 
    - to write and read 
    - when you're done.

In later practicums, we'll explore more of dplyr, 

  - such as operations based on two data sets.


##### Resources

- `dplyr` official stuff

  * package home [on CRAN](http://cran.r-project.org/web/packages/dplyr/index.html)
    - note there are several vignettes, with the [introduction](http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html) being the most relevant right now
    - the [one on window functions](http://cran.rstudio.com/web/packages/dplyr/vignettes/window-functions.html) will also be interesting to you now
  * development home [on GitHub](https://github.com/hadley/dplyr)
  * [tutorial HW delivered](https://www.dropbox.com/sh/i8qnluwmuieicxc/AAAgt9tIKoIm7WZKIyK25lh6a) (note this links to a DropBox folder) at useR! 2014 conference

[RStudio Data Wrangling cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), covering `dplyr` and `tidyr`. Remember you can get to these via *Help > Cheatsheets.* 

[Data transformation](http://r4ds.had.co.nz/transform.html) chapter of [R for Data Science](http://r4ds.had.co.nz)

[Excellent slides](https://github.com/tjmahr/MadR_Pipelines) on pipelines and `dplyr` by TJ Mahr, talk given to the Madison R Users Group.

Blog post [Hands-on dplyr tutorial for faster data manipulation in R](http://www.dataschool.io/dplyr-tutorial-for-faster-data-manipulation-in-r/) by Data School, that includes a link to an R Markdown document and links to videos



#### File I/O: Getting data into and out of R

##### File I/O overview

- We've been loading the Gapminder data 

  - as a data.frame from the `gapminder` data package. 

We haven't been explicitly writing any data or derived results to file. 

In real life, you'll bring rectangular data 

  - into and out of R all the time. 
  
Sometimes you'll need to do same for non-rectangular objects.

How do you do this? What issues should you think about?

##### Data import mindset

- Data import generally feels one of two ways:

  - "Surprise me!" 
    - This is the attitude you must adopt when you first get a dataset. 
    - You are just happy to import without an error. 
    - You start to explore. 
    - You discover flaws in the data and/or the import. 
    - You address them. 
    - Lather, rinse, repeat.
  * "Another day in paradise." 
    - This is the attitude when you bring in a tidy dataset 
    - you have maniacally cleaned in one or more cleaning scripts. 
    - There should be no surprises. 
    - You should express your expectations about the data 
    - in formal assertions at the very start of these downstream scripts.
  
In the second case, and as the first cases progresses, 

  - you actually know a lot about how the data is / should be. 
  
My main import advice: 

  - **use the arguments of your import function** 
  - **to get as far as you can,** 
  - **as fast as possible**. 

Novice code often has a great deal of 

  - unnecessary post import fussing around. 
  - Read the docs for the import functions 
    - and take maximum advantage of the arguments 
    - to control the import.

##### Data export mindset

- There will be many occasions when you need to write data from R. 

Two main examples:

  * a tidy ready-to-analyze data set that you heroically created from messy data 
  * a numerical result from data aggregation or modeling or statistical inference 

First tip: __today's outputs are tomorrow's inputs__. 

  - Think back on all the pain you have suffered importing data 
  - and don't inflict such pain on yourself!

Second tip: don't be too cute or clever. 

  - A plain text file that is readable by a human being in a text editor 
    - should be your default 
    - until you have __actual proof__ that this will not work. 
  - Reading and writing to exotic or proprietary formats 
    - will be the first thing to break 
    - in the future or on a different computer. 
  - It also creates barriers for anyone 
    - who has a different toolkit than you do. 
    - Be software-agnostic. 
    - Aim for future-proof and moron-proof.

How does this fit with our emphasis on dynamic reporting via R Markdown? 

  - There is a time and place for everything. 
  - There are projects and documents 
    - where the scope and personnel will allow you 
    - to geek out with `knitr` and R Markdown. 
  - But there are lots of good reasons why 
    - (parts of) an analysis 
    - should not (only) be embedded in a dynamic report. 
  - Maybe you are just doing data cleaning 
    - to produce a valid input data set. 
  - Maybe you are making a small 
    - but crucial contribution to a giant multi-author paper. Etc. 
  - Also remember there are other tools and workflows 
    - for making something reproducible.
    - I'm looking at you, [make](http://kbroman.github.io/minimal_make/).

##### Load the tidyverse, for readr and forcats 

- The main tidyverse package we will be using is readr, 

  - which provides drop-in substitutes for `read.table()` and friends. 
  
However, to make some points about data export and import, 

  - it is nice to reorder factor levels. 
  - For that, we will also use function 
    - from the forcats package.

```{r}
library(tidyverse)
```

##### Locate the Gapminder data

- We could load the data from the package as usual, 

  - but instead we will load it from tab delimited file. 
  - The gapminder package includes the data 
    - normally found in the `gapminder` data frame as a `.tsv`. 
    - So let's get the path to that file on *your* system.

```{r}
(gap_tsv <- system.file("extdata/gapminder.tsv", package = "gapminder"))
```

##### Bring rectangular data in

- The workhorse data import function of readr is `read_delim()`. 

  - Here we'll use a variant, `read_tsv()`, 
    - that anticipates tab-delimited data:

```{r}
gapminder <- NULL
gapminder <- read_tsv(gap_tsv)
str(gapminder, give.attr = FALSE)
```

For full flexibility re: specifying the delimiter, 

  - you can always use `readr::read_delim()`.

There's a similar convenience wrapper 

  - for comma-separated values, `read_csv()`.

The most noticeable difference between 

  - the readr functions and base 
    - is that readr 
    - does NOT convert strings to factors by default. 
  - In the grand scheme of things, 
    - this is better default behavior, 
    - although we go ahead and convert them to factor here. 
  - Do not be deceived -- in general, 
    - you will do less post-import fussing if you use readr.

```{r}
gapminder <- gapminder %>%
  mutate(country = factor(country),
         continent = factor(continent))
str(gapminder)
```

###### Bring rectangular data in -- summary

- Default to `readr::read_delim()` and friends. 

  - Use the arguments!

The Gapminder data is too clean and simple 

  - to show off the great features of readr, 
  - so I encourage you to check out the vignette on [column types](https://cran.r-project.org/web/packages/readr/vignettes/column-types.html). 
  - There are many variable types 
    - that you will be able to parse correctly upon import, 
    - thereby eliminating a great deal of post-import fussing.

##### Compute something worthy of export

- We need to compute something worth writing to file. 

Let's create a country-level summary 

  - of maximum life expectancy.

```{r}
gap_life_exp <- gapminder %>%
  group_by(country, continent) %>%
  summarise(life_exp = max(lifeExp)) %>%
  ungroup()
gap_life_exp
```

The `gap_life_exp` data frame 

  - is an example of an intermediate result 
  - that we want to store for the future 
    - for downstream analyses 
    - or visualizations.

##### Write rectangular data out

- The workhorse export function 

  - for rectangular data in readr is `write_delim()` and friends. 
  - Let's use `write_csv()` to get a comma-delimited file.

```{r}
write_csv(gap_life_exp, "./data/gap_life_exp.csv")
```

Let's look at the first few lines of `gap_life_exp.csv`. 

If you're following along, 

  - you should be able to open this file 
  - or, in a shell, use `head` on it.

```{r echo = FALSE, comment = NA}
"./data/gap_life_exp.csv" %>%
  readLines(n = 6) %>%
  cat(sep = "\n")
```

This is pretty decent looking, 

  - though there is no visible alignment or separation into columns. 
  - Had we used the base function `read.csv()`, 
    - we would be seeing row names and lots of quotes, 
    - unless we had explicitly shut that down. 
  - Nicer default behavior is the main reason we are using 
    - `readr::write_csv()` over `write.csv()`.

It's not really fair to complain about the lack of visible alignment. 

  - Remember we are ["writing data for computers"](https://twitter.com/vsbuffalo/statuses/358699162679787521). 
  - If you really want to browse around the file, 
    - use `View()` in RStudio 
    - but don't succumb to the temptation 
      - to start doing artisanal data manipulations in Excel
      - get back to R and construct commands 
      - that you can re-run the next 15 times you 
      - import/clean/aggregate/export the same data set. 
      - Trust me, it will happen.

##### Invertibility

- It turns out these self-imposed rules are often in conflict with one another

  * Write to plain text files
  * Break analysis into pieces: 
    - the output of script `i` is an input for script `i + 1`
  * Be the boss of factors: 
    - order the levels in a meaningful, usually non-alphabetical way
  * Avoid duplication of code and data

Example: after performing the country-level summarization, 

  - we reorder the levels of the country factor, 
    - based on life expectancy. 
  - This reordering operation is conceptually important 
    - and must be embodied in R commands stored in a script. 
  - However, as soon as we write `gap_life_exp` to a plain text file, 
    - that meta-information about the countries is lost. 
  - Upon re-import with `read_delim()` and friends, 
    - we are back to alphabetically ordered factor levels. 
  - Any measure we take to avoid this loss 
    - immediately breaks another one of our rules.

So what do I do? 

  - I must admit I save (and re-load) R-specific binary files. 
  - Right after I save the plain text file. [Belt and suspenders](http://www.wisegeek.com/what-does-it-mean-to-wear-belt-and-suspenders.htm).

I have toyed with the idea of writing import helper functions 

  - for a specific project, 
    - that would re-order factor levels in principled ways. 
  - They could be defined in one file and called from many. 
  - This would also have a very natural implementation within [a workflow where each analytical project is an R package](http://carlboettiger.info/2012/05/06/research-workflow.html). 
  - But so far it has seemed too much like [yak shaving](http://sethgodin.typepad.com/seths_blog/2005/03/dont_shave_that.html). 
  - I'm intrigued by a recent discussion of putting such information in YAML frontmatter (see Martin Fenner blog post [Using YAML frontmatter with CSV](http://blog.datacite.org/using-yaml-frontmatter-with-csv/)).

##### Reordering the levels of the country factor

- The topic of [factor level reordering is covered elsewhere](block029_factors.html), 

  - so let's Just. Do. It. 
  - I reorder the country factor levels 
    - according to the life expectancy summary 
    - we've already computed.

```{r}
head(levels(gap_life_exp$country)) # alphabetical order
gap_life_exp <- gap_life_exp %>%
  mutate(country = fct_reorder(country, life_exp))
head(levels(gap_life_exp$country)) # in increasing order of maximum life expectancy
head(gap_life_exp)
```

Note that the __row order of `gap_life_exp` has not changed__. 

I could choose to reorder the rows of the data frame 

  - if, for example, I was about to prepare a table to present to people. 
  - But I'm not, so I won't.

##### `saveRDS()` and `readRDS()`

- If you have a data frame AND 

  - you have exerted yourself to rationalize the factor levels, 
  - you have my blessing to save it to file 
    - in a way that will preserve this hard work upon re-import. 
  - Use `saveRDS()`.

```{r}
saveRDS(gap_life_exp, "./data/gap_life_exp.rds")
```

`saveRDS()` serializes an R object to a binary file. 

  - It's not a file you will able to 
    - open in an editor, 
    - diff nicely with Git(Hub), 
    - or share with non-R friends. 
  - It's a special purpose, limited use function 
    - that I use in specific situations.

The opposite of `saveRDS()` is `readRDS()`. 

  - You must assign the return value to an object. 
  - I highly recommend you assign back to the same name as before. 
    - Why confuse yourself?!?

```{r error = TRUE}
rm("./data/gap_life_exp")
gap_life_exp
gap_life_exp <- readRDS("./data/gap_life_exp.rds")
gap_life_exp
```

`saveRDS()` has more arguments, 

  - in particular `compress` for controlling compression, 
    - so read the help for more advanced usage. 
  - It is also very handy for saving non-rectangular objects, 
    - like a fitted regression model, 
    - that took a nontrivial amount of time to compute.

You will eventually hear about 

  - `save()` + `load()` and even `save.image()`. 
  - You may even see them in documentation and tutorials, 
    - but don't be tempted. Just say no. 
  - These functions encourage unsafe practices, 
    - like storing multiple objects together 
    - and even entire work spaces. 
  - There are legitimate uses of these functions, 
    - but not in your typical data analysis.

##### Retaining factor levels upon re-import

- Concrete demonstration of 

  - how non-alphabetical factor level order is lost 
    - with `write_delim()` / `read_delim()` workflows 
    - but maintained with `saveRDS()` / `readRDS()`.

```{r}
(
  country_levels <-
    tibble(original = head(levels(
      gap_life_exp$country
    )))
)
write_csv(gap_life_exp, "./data/gap_life_exp.csv")
saveRDS(gap_life_exp, "./data/gap_life_exp.rds")
rm(gap_life_exp)
# head(gap_life_exp) # will cause error! proving gap_life_exp is really gone
gap_via_csv <- read_csv("./data/gap_life_exp.csv") %>%
  mutate(country = factor(country))
gap_via_rds <- readRDS("./data/gap_life_exp.rds")
country_levels <- country_levels %>%
  mutate(via_csv = head(levels(gap_via_csv$country)),
         via_rds = head(levels(gap_via_rds$country)))
country_levels
```

Note how the original, post-reordering country factor levels 

  - are restored using the `saveRDS()` / `readRDS()` strategy 
  - but revert to alphabetical ordering using `write_csv()` / `read_csv()`.

##### `dput()` and `dget()`

- One last method of saving and restoring data deserves a mention: 

  - `dput()` and `dget()`. `dput()` 
    - offers this odd combination of features: 
    - it creates a plain text representation of an R object 
    - which still manages to be quite opaque. 
  - If you use the `file =` argument, 
    - `dput()` can write this representation to file 
    - but you won't be tempted to actually read that thing. 
    - `dput()` creates an R-specific-but-not-binary representation. 
  - Let's try it out.

```{r}
## first restore gap_life_exp with our desired country factor level order
gap_life_exp <- readRDS("./data/gap_life_exp.rds")
dput(gap_life_exp, "gap_life_exp-dput.txt")
```

Now let's look at the first few lines of the file `gap_life_exp-dput.txt`.

```{r echo = FALSE, comment = NA}
"gap_life_exp-dput.txt" %>%
  readLines(n = 6) %>%
  cat(sep = "\n")
```

Huh? Don't worry about it. 

  - Remember we are ["writing data for computers"](https://twitter.com/vsbuffalo/statuses/358699162679787521). 
  - The partner function `dget()` reads this representation back in.

```{r}
gap_life_exp_dget <- dget("gap_life_exp-dput.txt")
country_levels <- country_levels %>%
  mutate(via_dput = head(levels(gap_life_exp_dget$country)))
country_levels
```

Note how the original, post-reordering country factor levels 

  - are restored using the `dput()` / `dget()` strategy.

But why on earth would you ever do this?

The main application of this is [the creation of highly portable, self-contained minimal examples](http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example). 

  - For example, if you want to pose a question 
    - on a forum or directly to an expert, 
    - it might be required or just plain courteous 
    - to NOT attach any data files. 
  - You will need a monolithic, plain text blob 
    - that defines any necessary objects and has the necessary code. 
  - `dput()` can be helpful for producing 
    - the piece of code that defines the object. 
  - If you `dput()` without specifying a file, 
    - you can copy the return value from Console 
    - and paste into a script. 
  - Or you can write to file 
    - and copy from there 
    - or add R commands below.

##### Other types of objects to use `dput()` or `saveRDS()` on

- My special dispensation to abandon human-readable, plain text files 

  - is even broader than I've let on. 
  - Above, I give my blessing to store data.frames 
    - via `dput()` and/or `saveRDS()`, 
    - when you've done some rational factor level re-ordering. 
  - The same advice and mechanics apply a bit more broadly: 
    - you're also allowed to use R-specific file formats 
    - to save vital non-rectangular objects, 
    - such as a fitted nonlinear mixed effects model 
    - or a classification and regression tree.

##### Clean up

- We've written several files in this practicum. 

  - Some of them are not of lasting value or have confusing file names. 
  - I choose to delete them, 
    - while demonstrating some of the many functions R offers 
    - for interacting with the file system. 
  - It's up to you whether you want to submit this command or not.

```{r}
file.remove(list.files(pattern = "^gap_life_exp"))
```

##### Pitfalls of delimited files <a id="pitfalls"></a>

- If a delimited file 

  - contains fields where a human being has typed, 
    - be crazy paranoid because people do really nutty things. 
  - Especially people who aren't in the business of programming 
    - and have never had to compute on text. 
  - Claim: a person's regular expression skill 
    - is inversely proportional to the skill 
    - required to handle the files they create. 
  - Implication: if someone has never heard of regular expressions, 
    - prepare for lots of pain working with their files.

When the header fields 

  - (often, but not always, the variable names) 
    - or actual data contain the delimiter, 
    - it can lead to parsing and import failures. 
  - Two popular delimiters are the comma `,` and the TAB `\t` 
    - and humans tend to use these when typing. 
  - If you can design this problem away during data capture, 
    - such as by using a drop down menu on an input form, 
    - by all means do so. 
  - Sometimes this is impossible or undesirable 
    - and you must deal with fairly free form text. 
  - That's a good time to allow/force text to be protected with quotes, 
    - because it will make parsing the delimited file go more smoothly.

Sometimes, instead of rigid tab-delimiting, 

  - white space is used as the delimiter. 
  - That is, in fact, the default 
    - for both `read.table()` and `write.table()`. 
  - Assuming you will write/read variable names from the first line 
    - (a.k.a. the `header` in `write.table()` and `read.table()`), 
    - they must be valid R variable names ... 
    - or they will be coerced into something valid. 
  - So, for these two reasons, 
    - it is good practice to use "one word" variable names whenever possible. 
  - If you need to evoke multiple words, 
    - use `snake_case` or `camelCase` to cope. 
  - Example: the header entry for the field holding the subject's last name 
    - should be `last_name` or `lastName` 
    - NOT `last name`. 
  - With the `readr` package, 
    - "column names are left as is, 
    - not munged into valid R identifiers 
    - (i.e. there is no `check.names = TRUE`)". 
  - So you can get away with white space in variable names 
    - and yet I recommend that you do not.

##### References

- [Data import](http://r4ds.had.co.nz/data-import.html) chapter of [R for Data Science](http://r4ds.had.co.nz) by Hadley Wickham and Garrett Grolemund.

Nine simple ways to make it easier to (re)use your data by Ethan P White, Elita Baldridge, Zachary T. Brym, Kenneth J. Locey, Daniel J. McGlinn, Sarah R. Supp.

  * First appeared here: PeerJ PrePrints 1:e7v2 <http://dx.doi.org/10.7287/peerj.preprints.7v2>
  * Published here: Ideas in Ecology and Evolution 6(2): 1?10, 2013. doi:10.4033/iee.2013.6b.6.f <http://library.queensu.ca/ojs/index.php/IEE/article/view/4608>
  * Section 4 "Use Standard Data Formats" is especially good reading.
  
Tidy data by Hadley Wickham.

  * In the Journal of Statistical Software Vol 59 (2014), Issue 10, 10.18637/jss.v059.i10: <http://www.jstatsoft.org/article/view/v059i10>
  * PDF also available here: <http://vita.had.co.nz/papers/tidy-data.pdf>

#### Links

  - Jenny Bryan, RStudio software engineer
  - Stats Prof at U British Columbia
    - [https://twitter.com/JennyBryan](https://twitter.com/JennyBryan)

  