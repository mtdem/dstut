---
title: 'CWRU DSCI351-351m-451: Lab Exercise LE3'
subtitle: 'Normal Approximation, GGPlot, Functions, Data I/O'
author: "R. H. French, L. S. Bruckman, P. Leu, K. Davis, S. Cirlos"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    number_sections: true
    toc_depth: 6
    highlight: tango
  html_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 6, # the width for plots created by code chunk
  fig.height = 4, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300, 
  dev = 'png', # Makes each fig a png, and avoids plotting every data point
  # eval = FALSE, # if FALSE, then the R code chunks are not evaluated
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = TRUE, # if FALSE knitr won't display warning messages in the doc
  error = TRUE) # report errors
  # options(tinytex.verbose = TRUE)
```

```{=tex}
\setcounter{section}{3}
\setcounter{subsection}{0}
```
### LE3, 10 points.

Summary of points (use Cntrl + Shift + O for seeing sub-questions easily):-

Coding style: 1 point

-   LE3-1: 0.5 point
-   LE3-2: 1 point
-   LE3-3: 1 point
-   LE3-4: 2.5 points
-   LE3-5: 2 points
-   LE3-6: 2 points

```{r}
library(tidyverse)
```

#### Lab Exercise (LE) 3

------------------------------------------------------------------------

## LE3-1. Normal Approximation

According to the Health and Nutrition Examination Survey, the average height of women age 18- 24 is about 64.3 inches.

The standard deviation is about 2.6 inches.

Using the normal curve, estimate the percentage of women with heights

```{r}
percentage_multiplier <- 100
average_height <- 64.3
sd_height <- 2.6
intervalOne <- 60
intervalTwo <- 66
increment <- 1
q1a <- pnorm(intervalTwo, mean = average_height, sd = sd_height, lower.tail = TRUE)
q1a * percentage_multiplier

q1 <- pnorm(intervalOne, mean = average_height, sd = sd_height, lower.tail = TRUE)

q1b <- q1a - q1
q1b * percentage_multiplier 
```

(a) below 66 inches

ANSWER-\> `74.34%`

(b) between 60 and 66 inches.

ANSWER-\> `69.43%`

```{r}
q1LowerPercentile <- 0.1
q1HigherPercentile <- 0.9

q1c <- qnorm(q1LowerPercentile, mean = average_height, sd = sd_height, lower.tail = TRUE)
q1d <- qnorm(q1HigherPercentile, mean = average_height, sd = sd_height, lower.tail = TRUE)
q1c
q1d
```

(c) What height is the 10th percentile?

ANSWER-\> `60.97` inches

(d) What height is the 90th percentile?

ANSWER-\> `67.63` inches

## LE3-2. Normal Approximation for Probability

A coin is tossed 100 times.

Coin tosses follow what is called a binomial distribution (page 149 in OpenStats).

(a) What is the expected number of times that the coin comes up heads?

```{r}
numTosses <- 100
probabilityOfOneSide <- 0.5
expNumHeads <- numTosses * probabilityOfOneSide
expNumHeads
```

ANSWER-\> `50`

(b) What is the standard deviation for the number of times the coin comes up heads

```{r}
varNumHeads <- numTosses * probabilityOfOneSide * (1 - probabilityOfOneSide)
sdNumHeads <- sqrt(varNumHeads)
sdNumHeads
```

ANSWER-\> `5`

(c) If we were to consider a probability histogram for the number of times the coin came up heads, this could be approximated by the normal approximation.

Describe why we could make this approximation.

ANSWER-\> `n * p = n * q = numTosses * probabilityOfOneSide = numTosses * (1 - probabilityOfOneSide) = 50 > 5`. Thus, we can make a normal approximation.

(d) Use the normal approximation to estimate the chance of getting exactly 50 heads.
    Hint: you can find the percentage between 50.5 and 49.5.

```{r}
exactly50Heads <- pnorm(50.5, mean = expNumHeads, sd = sdNumHeads
                        ) - pnorm(49.5, mean = expNumHeads, sd = sdNumHeads)
exactly50Heads
```

ANSWER \<- `0.08`

(e) Use the normal approximation to estimate the chance of getting between 45 and 55 heads inclusive.

```{r}
between45And55Inc <- pnorm(55, mean = expNumHeads, sd = sdNumHeads
                           ) - pnorm(45, mean = expNumHeads, sd = sdNumHeads)
between45And55Inc
```

ANSWER \<- `0.68`

(f) Use the normal approximation to estimate the chance of getting between 45 and 55 heads exclusive

```{r}
between45And55Exc <- pnorm(54, mean = expNumHeads, sd = sdNumHeads
                           ) - pnorm(46, mean = expNumHeads, sd = sdNumHeads)
between45And55Exc
```

ANSWER \<- `0.58`

## LE3-3. Normal Approximation for Probability

We replace the 50/50 coin with a weighted coin

-   that comes out to be heads 10% of the time and tails 90% of the time.

We toss it 100 times.

(a) What is the expected number of times that the coin comes up heads?

```{r}
weightedProbOfHeads <- 0.1
wExpNumHeads <- numTosses * weightedProbOfHeads
wExpNumHeads
```

ANSWER \<- `10`

(b) What is the standard deviation for the number of times the coin comes up heads

```{r}
wVarNumHeads <- numTosses * weightedProbOfHeads * (1 - weightedProbOfHeads)
wSdNumHeads <- sqrt(varNumHeads)
wSdNumHeads
```
ANSWER-\> 5

(c) Can we still use the normal approximation in this case?

ANSWER \<- `n * p = numTosses * weightedProbOfHeads = 10 > 5$$; $$n * q = numTosses * (1 - weightedProbOfHeads) = 90 > 5`
            Thus, we can still use normal approximation

(d) Estimate the chance of getting exactly 10 heads.

```{r}
exactly10Heads <- pnorm(10.5, mean = wExpNumHeads, sd = wSdNumHeads
                        ) - pnorm(9.5, mean = wExpNumHeads, sd = wSdNumHeads)
exactly10Heads
```

ANSWER \<- `0.08`

## LE3-4. Normal Approximation for Data Analysis

A list of exam scores is provided.

-   Read these scores in as a data frame.
-   Rename the column to `scores`.

For the exam scores, calculate

-   the mean and
-   standard deviation

```{r}
# read in the dataset
filepath <- "data/exam_scores.csv"
exams <- read.table(file = filepath)
# rename the variable to 'scores'
scoresStr <- "scores"
names(exams) = c(scoresStr)

meanScores <- mean(exams$scores)
sdScores <- sd(exams$scores)
meanScores
sdScores
```

(a) What is the mean of the exam data?

ANSWER-\> `72.2`

(b) What is the standard deviation of the exam data?

ANSWER-\> `11.6`

(c) The first student scored an 84. What percentage of students are equal to or below this score?

```{r}
firstStudentScore <- exams$scores[1]
leFirstStudentScore <- pnorm(firstStudentScore, meanScores, sdScores, lower.tail = TRUE)
leFirstStudentScore * percentage_multiplier
```

ANSWER-\> `84.56`

(d) Now use the normal approximation.

Use the pnorm function to determine what percentage of students would be expected to be below this score?

```{r}
ltFirstStudentScore <- pnorm((firstStudentScore + 1), meanScores, sdScores, lower.tail = TRUE)
ltFirstStudentScore * percentage_multiplier
```

ANSWER-\> `86.52`

(e) Plot a normalized histogram using ggplot.

-   Use `geom_histogram(aes(y = ..density..))` in gg plot to plot a normalized histogram.

Plot a normalized Gaussian curve on top for comparison.

-   Use `stat_function(fun = dnorm)` with appropriate arguments

```{r}
## Input: vector of scores data, desired mean, desired sd
## Output: data frame of scores data with corresponding normalized data
NormalFrame <- function(scoresVector, mean, sd) {
  normData <- dnorm(scoresVector, mean, sd)
  frameData <- data.frame(score = scoresVector, normalized = normData)
  return(frameData)
}

## Input: NormalFrame data frame
## Output: histogram + gaussian curve of norm data
NormalPlot <- function(scoreData) {
  scoreplot <- ggplot(data = scoreData, mapping = aes(x = score, y = normalized)
                      ) + geom_histogram(alpha = 0.4, stat ='identity'
                      ) + labs(title = "Normalized Scores") + xlim(40, 100
                      ) + stat_function(data = scoreData, fun = dnorm) + geom_line()
  return(scoreplot)
}

library(ggplot2)
standardData <- NormalFrame(exams$scores, mean = meanScores, sd = sdScores)
standardPlot <- NormalPlot(standardData)
standardPlot
```

(f) You want to curve or renormalize the scores

-   so that the mean is 77 and
-   the standard deviation is 10.

Create a new column in the data frame with the `curved_scores` which are rounded to the nearest integer.

Create a new ggplot with a histogram with the curved scores.

```{r}
meanCurve <- 77
sdCurve <-  10
curveData <- NormalFrame(exams$scores, mean = meanCurve, sd = sdCurve)
curvePlot <- NormalPlot(curveData)
curvePlot
```

## LE3-5. Text Mining of Song Lyrics:

In LE2-3c-d, you created word clouds for Elton John and Eminem. As in LE2, the dataset for this assignment is a collection of the information and lyrics from every top 100 billboard song since 1965. Let's modify that code so that it is more flexible and extensible. The modified code will work with

-   an arbitrary list of artists `artists_select` and
-   an arbitrary number of `max_wordcloud_words`

```{r}
library(tidytext) 
library(tm)  # the Text Mining Package
library(NLP) # the Natural Language Processing package
library(wordcloud)
library(magrittr)
library(dplyr)
# load in the dataset 
billboard_df <- read.csv('./data/billboard_lyrics_1964-2015.csv') %>%
  as.data.frame()
```

### LE3-5a. Creating Extensible and Flexible Code

Write a function, named `GenerateWordCloud` that creates a wordcloud

-   for each artist in an arbitrarily chosen list of artists with `max_wordcloud_words`
-   So this function needs to work for 1, 2, 3 or more artists
-   The word cloud should not include any `stop_words` as in LE#2
-   Write your code below for the `GenerateWordCloud` function

```{r}
data("stop_words")
# Use VCorpus(VectorSource(word) in wordcloud to eliminate warnings
GenerateWordCloud <-
  function(artists_select, billboard_df, max_wordcloud_words) {
    # This function generates word clouds for each artist in the list artists_select
    # with the maximum number of words max_wordcloud_words
    #
    # Write your code here for the GenerateWordCloud function
    #
    for(artist in artists_select) {
      billboard_artist <- billboard_df[billboard_df$Artist == artist, ]$Lyrics
      row.names(billboard_artist) <- NULL
      artist_words <- tibble(text = billboard_artist)
      artist_words <- artist_words %>% unnest_tokens(word, text) %>% anti_join(stop_words) 
      artist_words_df <- as.data.frame(table(artist_words))
      names(artist_words_df) <- c("word", "freq")
      artist_words_df <- artist_words_df[order(artist_words_df$freq, decreasing = T), ]
      row.names(artist_words_df) <- NULL
      wordcloud_words <- artist_words_df$word[1:max_wordcloud_words]
      wordcloud_freq <- artist_words_df$freq[1:max_wordcloud_words]
      artist_wordcloud <- wordcloud(wordcloud_words, wordcloud_freq) 
      artist_wordcloud
    }
  }

```

### LE3-5b Testing your function

When writing functions, it is useful to have some test scripts to make sure that the function works under a variety of test cases. Use the following test cases to make sure the code is working properly on different cases.

Test your function for 2 arbitrary artists.

```{r}
max_wordcloud_words <- 30
artists_select <- c("elton john","eminem")
GenerateWordCloud(artists_select, billboard_df, max_wordcloud_words)
```

Test your function on 1 artist.

```{r}
max_wordcloud_words <- 40
artists_select <- c("elton john")
GenerateWordCloud(artists_select, billboard_df, max_wordcloud_words)
```

And on 3 artists.

```{r}
max_wordcloud_words <- 20
artists_select <- c("cher", "madonna", "billy joel")
GenerateWordCloud(artists_select, billboard_df, max_wordcloud_words)
```

## LE3-6. Acrylic Hardcoats: Reading in data from csv files

You will practice reading in data and writing efficient code.

You will create a color files dataframe by going into multiple folders and concatenating data from multiple .csv files into one large data frame.
 
A good practice in developing code is to write out pseudocode, which serves an outline for your code. The pseudocode has been written out here for you.

Write a function that will

-   Go into a particular folder.\
-   Get all the files in that folder using `files <- list.files(path = folder_name)`
-   Read the .csv files
-   Bind the .csv files together by row.
-   Read in the `data_sample_key` dataframe with `data_sample_key <- read.csv('data/acryhc-key.csv')`
-   The dataframes can be merged using `merge(x = data_total, y = data_sample_key, by.x = 'ID', by.y = 'Sample.Number')` as the `ID` in the color files are matched to the `Sample.Number` in `data_sample_key`
-   Add an additional column to the dataframe with the `step_number`

Now use your function on all all color datafiles in the `./data/color` folder. Add a `for` loop or use `map_dfr` to apply the same function over multiple files.

Your final data frame should be `data_color_all` and have 775 observations with 10 variables.

Key: Do not duplicate code.

As you are developing your code, initially test out the function on a single file. Then, test it on a list of files in one folder. Then, test it on all folders. This will make debugging easier.

If you write your code efficiently, you should be able to code everything in only 30 lines of code or less.

```{r}
library("dplyr")                                    # Load dplyr package
library("plyr")                                     # Load plyr package
library("readr")                                    # Load readr package

## Perform this:
## -   Go into a particular folder.
## -   read data/acryhc-key.csv as data_sample_key
## -   Get all the .csv files in that folder
## -   Read the .csv files
## -   Bind the .csv files together by row.
## -   merge row-binded df with data_sample_key obtained from on Sample.Number~ID
## -   Add step number
ColorToAcrylic <- function(folder, subfolder, data_sample_key) {
    dirSeperator <- "/"
    stepStr <- substr(subfolder, 5, 5)
    stepNum <- as.integer(stepStr)
    subfolderpath <- paste(folder, subfolder, dirSeperator,
                           sep = dirSeperator)
    files_t <- list.files(path = subfolderpath,
                          pattern = "*.csv",
                          full.names = TRUE) %>%
      lapply(read_csv) %>%
      bind_rows
    data_total <- as.data.frame(files_t)
    data_total %>% merge(x = data_total,
                         y = data_sample_key, 
                         by.x = 'ID', 
                         by.y = 'Sample.Number')
    data_total$step_number <- stepNum
    return(data_total)
}

## Perform ColorToAcrylic (primary logic) on entire color folders
## return df containing all the data from the step<i> folder for i = 1..4
MapColorsToAcrylicHardcoats <- function(folder) {
  subfolders <- list.files(path = folder)
  data_sample_key <- read.csv('./data/acryhc-key.csv')
  data_color_all <- data.frame()
  for (subfolder in subfolders) {
    data_color <- ColorToAcrylic(folder, subfolder, data_sample_key)
    data_color_all <- bind_rows(data_color_all, data_color)
  }
  return(data_color_all)
}

## Execute Code
colorFolder <- './data/color/'
data_color_all <- MapColorsToAcrylicHardcoats(colorFolder)
summary(data_color_all)
```

#### Links

<http://www.r-project.org>

<http://rmarkdown.rstudio.com/>
